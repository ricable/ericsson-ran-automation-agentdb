---
complexity_score: 41.0
converted_at: '2025-08-13T06:58:03.701898Z'
file_type: html
format: html
image_processing_enhanced: true
images_extracted: 55
images_saved: 3
original_path: 371_22112-IPM10141_100Uen.V.html
pictures_extracted: 55
processing_method: docling_multimodal
quality_score: 9.0
source_file: 371_22112-IPM10141_100Uen.V.html
source_zip: en_lzn7931071_r38f.zip
tables_extracted: 15
zip_image_types:
- .svg
- .png
- .gif
- .jpg
- .bmp
zip_images_total: 2796
---

# 

Solution Guideline for Real-Time Media Services in Public RAN

Contents

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

|   1 | Introduction                                         |
|-----|------------------------------------------------------|
| 1.1 | General                                              |
| 1.2 | Scope                                                |
| 2   | Principles and Guidelines                            |
| 2.1 | Solution Overview                                    |
| 2.2 | Network Slicing                                      |
| 2.3 | NR Spectrum Assets                                   |
| 2.4 | TCC Toolbox Features                                 |
| 3   | RAN Feature List                                     |
| 4   | Example Network Designs                              |
| 4.1 | Scenario 1: Dedicated Network Slice for Critical IoT |
| 4.2 | Scenario 2: Shared Network Slice for Critical IoT    |
| 5   | Solution Guideline Change History                    |

# 1 Introduction

## 1.1 General

The Solution Guideline for Real-Time Media Services in Public RAN introduces design principles for the deployment of Time-Critical Communication (TCC) services in the public RAN using NR Standalone (NR SA). The guideline considers performance requirements for the real-time media use case family, including cloud AR/VR/XR, cloud gaming or video conferencing services.

The guideline also provides configurations differentiating TCC from other services running on the same device. The assumption is that TCC services are not only accessible through dedicated devices but also through the NR SA capable smartphones of eMBB users. The TCC services are made available to eMBB users in the public RAN through a dedicated network slice.

This document provides the following types of information:

- Proposals on the use of network slices for Critical IoT solution deployments.
- Service-adaptive configuration of RAN features.
- High-level information on relevant network design aspects for technical sales managers and solution architects.

## 1.2 Scope

The document mainly focuses on the following areas:

- Public RAN using NR Standalone (SA) Option 2
- Network slice deployment options and service traffic steering on UEs
- TCC toolbox features for NR RAN
- Critical IoT Solution Design Considerations and Example Configurations

# 2 Principles and Guidelines

## 2.1 Solution Overview

Many emerging 5G use cases are time-critical in nature, demanding consistent low latency and high reliability at a guaranteed level. The Critical IoT solution addresses such Time-Critical Communication (TCC) needs of communications service providers. TCC delivers data within the specific time window required by the application. A system designed for enhanced Mobile Broadband (eMBB) maximizes data rates without any guarantee regarding latency. In contrast, TCC is designed to secure data delivery within specific latency bounds (X ms) with the desired reliability (Y percent). Depending on the service requirements, X ranges from tens of milliseconds to 1 ms latency, and Y ranges from 99 % to 99.999 % reliability.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 1   The Difference Between eMBB and Time-Critical Communication

Note: Although this release of the Critical IoT solution can provide consistent low latency, bounded latency is not yet supported. The solution leverages the Ericsson 5G network infrastructure and key technologies to meet a wide range of low latency and high reliability requirements.

Innovative services, including real-time media, industrial control, remote control, and mobility automation can be enabled with TCC. This release of the solution guideline focuses on the public RAN deployments covering services from the real-time media use case family including for example cloud gaming, cloud VR, cloud AR, or conferencing services.

The table below presents some high-level performance figures for real-time media services in 5G networks.

Note: The performance values can vary between service platforms and applications. For example, many cloud games require an average downlink throughput of 15 Mbps. But certain fast-paced games with complex graphics might peak up to higher values, while others still work well at data rates below 10 Mbps. Many real-time media services also require a minimum downlink or uplink throughput for their operation which is not reflected in the table. Moreover, for cloud AR/VR/XR services, the required throughput and latency performance also depends on the distribution of local versus cloud-assisted processing techniques in the split architecture.

| Use Cases          | DL Bitrates(Mbps)                     | UL Bitrates(Mbps)   | End-to-end Latency/RTT (ms)   |
|--------------------|---------------------------------------|---------------------|-------------------------------|
| Cloud AR           | ≤ 20                                  | ≤ 1                 | ≤ 100                         |
| Cloud VR           | ≤ 60                                  | ≤ 2                 | ≤ 100                         |
| Cloud XR           | ≤ 120                                 | ≤ 2                 | ≤ 80                          |
| Cloud Gaming       | ≤ 15                                  | < 1                 | ≤ 100                         |
| Video Conferencing | ≤ 4 (single call) / ≤ 10 (group call) | ≤ 4                 | ≤ 200                         |
| XR Conferencing    | ≤ 10                                  | ≤ 6                 | ≤ 150                         |

Realizing TCC involves an end-to-end approach across all involved network domains (RAN, Transport and 5GC) to give the desired output.

A multitude of different aspects need to be considered for the deployment of TCC services in the public RAN including for example:

- End-to-end network slice and related function deployment in the 5GS.
- User- and service-specific customization of TCC Toolbox features.
- Radio environment and spectrum availability in the RAN.
- Available network topology.
- Service monetization based on subscriptions and policies.
- Device ecosystem and support for service traffic categorization and steering.

The figure below shows the high-level solution architecture addressed in this guideline.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 2   Solution Architecture

Network slicing principles are applied to address the performance and Quality of Service (QoS) differentiation requirements of TCC services in the public RAN. They also help to facilitate the monetization and use of TCC services based on the provided subscriptions and policies in the 5GC network. To support the differentiated treatment of TCC and other services, a dedicated CIoT network slice is introduced to apply and customize the required TCC toolbox features across the public infrastructure.

Note: The use of a common, shared network slice for TCC and eMBB services is considered as a deployment alternative which might initially be used before introducing dedicated network slices to realize Critical IoT solutions.

This release of the solution guideline mainly focuses on the application of certain available TCC Toolbox features in the Ericsson RAN to meet the end-2-end performance requirements in 5G networks.

To allow UEs to select the correct network slice depending on the requested service, they must support the provisioning of UE Route Selection Policies (URSP).

Note: If UEs are dedicated to a single network slice URSP support might not be required.

For a general overview of the Ericsson Critical IoT Solution for the public RAN and the different TCC use case families, see Ericsson Critical IoT for Public RAN All-In-One User Guide in the Ericsson Critical IoT for Public RAN CPI library.

## 2.2 Network Slicing

A network slice is defined by Ericsson as a logical network serving a defined business purpose or customer, consisting of all required network resources configured together. It is associated with the required resources, network functions and node or feature configurations providing the necessary performance characteristics for the services using the network slice.

The network slice and related services are made available to users by means of subscriptions. The services can therefore be monetized through a network slice-related subscription besides being policed and charged individually, for example, depending on provided minimum data rate guarantees or other performance criteria.

In the Ericsson RAN the corresponding network slice identifier, the Single Network Slice Selection Assistance Identifier (S-NSSAI), can be used to control the resource usage, like the radio spectrum share, customize TCC toolbox and other NR RAN features, or filter the RAN performance counters for the purpose of observability.

The S-NSSAI-based resource control and feature customization applies to all services within the network slice. If multiple services within the same network slice must be differentiated with respect to the resource control or feature customization, the S-NSSAI must be used together with the corresponding service or QoS flow identifier, which is called 5G Quality of Service Identifier (5QI).

In short, two levels of control and customization are available in the Ericsson RAN which is shown in the figure below.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 3   Resource Control for Network Slices and Services in RAN

For slice 1 the same set of RAN resources and features including respective configurations is commonly applied to all service flows (5QIs) within the network slice.

For slice 2 different sets of RAN resources, features or feature configurations are individually applied to each service flow in the network slice.

The RAN resources and features must be configured to meet the performance characteristics required by the services.

Note: The QoS resources related to scheduling or congestion control in the RAN are always configured per 5QI and can be customized in a slice-specific way.

In the 5GC network the network slice is associated with a certain deployment option for the required 5GC functions as shown in the figure below. 5GC functions can be shared across network slices or be dedicated to a specific network slice.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 4   Resource Control for Network Slices and Services in 5GC

In the example, the 5GC functions related to mobility, session, subscription, policy, or slice management are shared across all network slices while the functions related to user plane forwarding are dedicated to individual network slices.

The user plane forwarding (UPF) function can either be deployed at central locations together with the shared functions or at locations close to the RAN.

Deploying the UPF close to the RAN is beneficial to optimize throughput and latency performance of services accessible through the connected Data Networks (DN). It also offloads traffic from the central parts of the transport network that interconnects the RAN with the 5GC network.

The S-NSSAI and Data Network Name (DNN) controls the resource usage in the 5GC in terms of selecting the session management function (SMF) and the user plane forwarding (UPF) function for the UE requested network slice. The selection information is either provided by the UE during PDU session setup or derived from corresponding UE subscription information through the user data management function (UDM).

A UE can be subscribed to a single or multiple network slices and corresponding services.

For more information related to network slicing in the Ericsson RAN, see NR SA Network Slicing Guideline.

### 2.2.1 Slice Selection and Service Traffic Steering

According to 3GPP, network slice selection is done primarily based on the Single Network Slice Selection Assistance Identifier (S-NSSAI), which can be associated with one or more network slice instances.

The UE provides the S-NSSAI for network slice selection to the network during PDU Session setup if it was configured with a corresponding Network Slice Selection Policy (NSSP). Otherwise, the 5G core network selects a default network slice on behalf of the UE based on the available subscription information.

If a UE only uses one network slice, it does not need to be provisioned with NSSPs. If it uses multiple network slices, the NSSPs are mandatory to steer the PDU session carrying the application or service traffic to the intended network slice.

The NSSPs on the UE can be realized either by means of (vendor specific) local configuration or signaling of UE Route Selection Policies (URSP) from the Policy Control Function (PCF) in the 5GC network.

The figure below shows a traffic steering example with two latency sensitive services having different performance characteristics. Each service is associated with an own trafﬁc category which is mapped to a different PDU Session and QoS ﬂow to allow QoS differentiation in the 5G network.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 5   URSP-Based Traffic Steering with Multiple TCC Services

The service flow using 5QI 80 might be used for an AR service and is associated with the traffic category called Very Low Latency (VLL). The traffic of the service is steered into the same network slice that is also used for the 5QI 7 service in the Low Latency category, but it uses a different DNN and PDU session.

In Ericsson RAN, each QoS flow is carried by an own Data Radio Bearer (DRB) connecting the UE to the RAN. The multiplexing of multiple QoS flows to a single DRB is not supported. Moreover, UEs currently support a maximum of 8 DRBs.

For initial NR SA deployments, dedicated PDU sessions with a single QoS flow, that means the default QoS flow, might be used for each traffic category. This reduces the need for network-initiated session modification procedures requiring Policy and Charging Control (PCC) rules and Packet Filters to be provisioned for Uplink and Downlink classification in the UE and User Plane Function (UPF).

Note: In the example above, IMS-based traffic is not mapped to a particular traffic category and uses a PDU session with multiple QoS flows.

Application traffic belonging to different traffic categories should not be transported through a shared QoS flow (5QI) and DRB because then no QoS differentiation in the 5G network is possible.

For more information, see E2E QoS Guideline.

### 2.2.2 Slice Deployment

There is no golden rule to decide upon using dedicated network slices for each traffic category or a shared network slice if the performance requirements for the TCC services can be fulfilled by applying the needed functions and node configurations for QoS differentiation in the different 5G network domains.

Two network slice deployment options for the Critical IoT solution in the public RAN are shown in the figure below.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 6   Network Slice Deployment Options

Option 1 uses a dedicated network slice for all TCC services besides the eMBB slice that is running the IMS-based Voice over NR (VoNR) and broadband data services. In contrast, option 2 runs all services together in the eMBB network slice using separate PDU sessions. With respect to the 5GC function deployment, both options are identical in that they use a dedicated UPF located close to the RAN to minimize latency.

Note: With option 2, URSP rules are only required if the UE does not know the DN Names by other configuration means to steer the service traffic into corresponding PDU Sessions.

Option 1 will be the focus in this guideline since it provides more flexibility in terms of setting up resource control, feature configuration or performance observability in the RAN.

For example, in case multiple TCC services with slightly different performance characteristics are transported in dedicated PDU Sessions and QoS flows, option 1 allows applying a basic slice-level setup by use of the S-NSSAI. The basic setup is applied to all TCC services in the network slice. A service-specific setup by use of the 5QI can be applied in addition to the basic setup but only to those TCC services requiring further QoS differentiation.

With option 1, it is also possible to monitor the RAN performance for TCC on slice-level, meaning per S-NSSAI, without requiring the aggregation of the performance figures from the specific services in the network slice which would be required with option 2.

Similar benefits might also show up in the 5GC network related to subscription handling. A user can be subscribed to the Critical IoT network slice providing a range of TCC services instead of subscribing to each TCC service on an individual basis. However, for initial Critical IoT deployments offering a single TCC service this might be a negligible advantage.

Other factors favorizing the use of a dedicated slice include security aspects, if network slices must be isolated from each other, or differences in the service area setup influencing access and mobility aspects.

Nevertheless, Option 2 can be used as a starting point for early Critical IoT deployments in the public RAN and be migrated to Option 1 when the necessary network slicing tools and operations are in place.

When using a dedicated network slice for the Critical IoT solution and related TCC service offerings, the network slice must be deployed homogenously in the RAN. This means, that the same set of network slices is defined across all NR cells on all frequency layers throughout the network to avoid mobility or load distribution problems. A homogenous deployment example for the Critical IoT network slice is shown in the figure below.

Note: The same set of network slices must be deployed across all NR Cells in a RAN tracking area (TA).

If required, the access to specific cells or frequency layers can be controlled by RAN and 5GC features including NR Mobility, NR Traffic Steering and Presence Reporting Area (PRA).

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 7   Network Slice Deployment in RAN

The penetration of the Critical IoT network slice throughout the mobile network depends on the rollout plans of the TCC services. This implies that the network slice might not be available in all geographical areas of a PLMN if the TCC services are not offered there. This is similar to Fixed Wireless Access deployments where the UE access to the network is typically confined to specific geographical areas, that means RAN Tracking Areas.

Different Critical IoT network slices can be introduced to address different use case families and deployment scenarios. The network slices can be identified with a standard or non-standard S-NSSAI. A standard S-NSSAI is composed of a 3GPP defined SST value without a Slice Differentiator (SD) value must be used if inter-operability between MNO networks is required for the services in the network slice. For operator-specific service and network slice deployments, a non-standard S-NSSAI can be used. The non-standard S-NSSAI is composed either of a 3GPP defined SST value with a SD value or an operator defined SST value greater than or equal to 128 with or without a SD value.

Following 3GPP defined SST values can be used for Critical IoT network slices:

- SST: 2 Ultra-reliable low latency communication (URLLC) For services requiring transmissions with high reliability and low latency including, for example public safety, remote control, or smart grid operations.
- SST: 6 High data rate and low latency communications (HDLLC) For services requiring high data rates with low latency specifically targeting extended reality and multi-modal communication services including, for example cloud AR/VR, video or XR conferencing.

The table below shows some network slice examples addressing different use cases and RAN infrastructure deployments.

| Slice Name   | Deployment    | Use Case Family                         | SST Value   | SD Value    |
|--------------|---------------|-----------------------------------------|-------------|-------------|
| CIoT         | Public RAN    | Real-Time Media                         | 6           | None or ≥ 0 |
| Robust CIoT  | Public RAN    | Remote Control                          | 2           | None or ≥ 0 |
| Robust CIoT  | Dedicated RAN | Industrial Control, Mobility Automation | ≥ 128       | ≥ 0 or None |

For public infrastructure deployments, the SST value 2 can be used for remote control uses cases. The corresponding network slice is named Robust CIoT to emphasize the additional need for transmission reliability and availability. The SST value 6 can be used for the identifier of a CIoT network slice supporting real-time media use cases. The S-NSSAI value can be constructed with or without a SD value dependent on required inter-operability between mobile networks.

For dedicated infrastructure deployments addressing TCC services in industry or enterprise environments, a non-standard S-NSSAI with an operator-defined SST value might be preferred to identify a Robust CIoT network slice.

## 2.3 NR Spectrum Assets

The figure below shows the spectrum assets that are available for TCC in 5G NR Standalone deployments. Depending on the TCC use case family and required coverage, the Critical IoT solution can be realized either with a dedicated or public RAN infrastructure.

For the real-time media use case family discussed in this solution guideline, the TCC services require wide area coverage and are realized with public RAN infrastructures which mobile network operators already built out for their eMBB data and VoNR service offerings.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 8   5G NR Spectrum Assets for TCC

The real-time media use case services require moderate to high throughput for which the use of Mid Band frequencies in FR1 or High Band frequencies in FR2 is preferred.

In FR1, peak throughput can be achieved by moving UEs to the capacity layer frequencies or configuring NR Carrier Aggregation with several SCell frequencies based on the supported UE capabilities. RAN features like NR Traffic Steering help to optimize the UEs throughput based on the available frequencies providing coverage.

However, a certain trade-off must be made between latency, reliability, and throughput to accommodate the performance requirements of these TCC services in the best possible way. In consequence, the use and configuration of RAN features optimizing for throughput must be balanced against those optimizing for latency and reliability.

For example, if users only run TCC services and sufficient throughput is provided by the PCell, NR carrier aggregation is not required. If users run TCC and eMBB services together, it might be beneficial to configure service-specific NR carrier aggregation to offload eMBB traffic to SCells while TCC traffic continues to be scheduled to the PCell only. If NR carrier aggregation is enabled for TCC services, the TCC service performance might be impacted by the additional E5 link latency when scheduling TCC traffic to external SCells.

Moreover, intra- or inter-frequency mobility triggered by NR Mobility and NR Traffic Steering features impact the service performance, for example, due to gap periods required by the UE to perform measurements. Depending on the impact, certain mitigations can be taken. For example, the use of certain frequencies for NR Carrier Aggregation and NR Traffic Steering can be disallowed or the periodicity to optimize the UEs cell set with NR Traffic Steering can be customized.

Note: Idle mode cell reselection procedures provided by the NR Mobility feature can be used to steer UEs to the desired frequencies without causing measurement related performance impacts.

Depending on which service is currently active on the UE and considered most important, the RAN features available for connectivity, mobility or traffic steering might temporarily be disabled or configured in a service-specific way through the UE Grouping Framework feature. This might impact the performance of other services running on the UE.

For example, the use of NR Carrier Aggregation for eMBB or TCC services might be disabled while the Voice over NR (VoNR) is running on the UE.

The use of TDD patterns in NR FR1 and FR2 is subject to certain restrictions imposed by the spectrum deployment to avoid interference. A certain TDD pattern must be deployed homogenously across all NR cells using a certain frequency. Moreover, with NR Carrier Aggregation the pattern must be aligned across all component carrier frequencies.

Thus, the TDD pattern cannot be arbitrarily changed from a downlink heavy to a more balanced or uplink heavy pattern per cell or geographical area unless the frequency is isolated from others which is more applicable to dedicated RAN than public RAN deployments.

Note: The TDD pattern is a cell-specific configuration and impacts all services using the NR cell, including eMBB, VoNR, and TCC.

Related Information

- 5G Mobility and Traffic Management Guideline
- NR Traffic Steering Solution Guideline
- Control Channel Guideline NR

## 2.4 TCC Toolbox Features

Ericsson has identified six major causes of latency and interruption in mobile networks which are shown in the figure below. The amount each cause contributes to the latency or availability is not fixed and depends on the level of mobility, congestion, and traffic load in the network besides other factors.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 9   TCC Toolbox

The causes can be addressed with a unique toolbox of features which must be utilized in an efficient way to support the coexistence with other services sharing the same network.

The toolbox for TCC consists of RAN-specific tools and E2E tools that need to be selected and combined based on the performance requirements of the deployed services.

The following sub-sections will focus on certain tools or features available for public RAN deployments. The goal is to demonstrate their service-adaptive use and mutual inter-actions in the RAN domain.

The application of the TCC toolbox features depends on the implemented QoS flow approach which consists of:

- Service-optimized QoS flows Should be established for TCC services with known performance requirements in terms of latency, error rates, or throughput among others. The TCC toolbox features can be tuned according to the specific needs of the service. The VoNR service flow carrying user plane voice (5QI 1) is one example of a service-optimized QoS flow. A service-optimized QoS flows relates to a dedicated QoS flow in the PDU session established by a service.
- Generic-optimized QoS flows Should be established when performance requirements of the TCC services are unknown. The 5G system applies a generic QoS treatment to accommodate a range of TCC services or multiple traffic types from a single TCC service without relying on a tight coupling between the services and the network. This means, that the TCC toolbox features can only be tuned to an extent that equally satisfies all TCC services or traffic types carried by the QoS flow. A generic-optimized QoS flows relates to the default QoS flow in the PDU session established by a service.

The assumption is that many TCC services in the real-time media use case family are initially handled through the generic-optimized QoS flow approach like shown in the figure below.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 10   Generic-Optimized QoS Flow for TCC applications

In this example, the TCC service uses the default QoS flow for multiple traffic types, like audio, video, signaling or pose tracking. This might be due to following reasons:

- The number of Data Radio Bearer (DRBs) supported by UEs which might limit the setup of many service-optimized QoS flows.
- The lack of network-provided interfaces to expose and communicate the performance requirements of service applications to the 5G network.
- To avoid the administrative overhead to maintain local Policy and Charging Control (PCC) rules in the 5GC, to detect and separate different traffic types for a multitude of different services. The mobile operator might not be in control of all services deployed on top of the mobile network.

The use of network-provided interfaces, for example the Network Exposure Function (NEF) N33 or Policy Control Function (PCF) N5 interface, allow TCC application functions (AFs) to trigger the modification of existing QoS flows or PDU sessions. This allows transforming a generic-optimized QoS flow into one or several service-optimized QoS flows which facilitates the service-optimized configuration of TCC toolbox features and the differentiated treatment of the corresponding AF traffic types. In the absence of network-provided interfaces, local PCC rules might be configured through the 5GC PCF to classify AF traffic and trigger the setup of dedicated QoS flows.

The 5GC network might also be able to detect certain application capabilities by means of traffic inspection, for example whether the application supports scalable congestion control with Low Latency, Low Loss, Scalable Throughput (L4S). If L4S support is detected, the 5GC can trigger the setup of a dedicated QoS flow and the installation of related packet filters in the UE and UPF to steer L4S traffic into the dedicated QoS flow. This allows the network to optimize the service performance specifically for application traffic supporting L4S like shown in the figure below.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 11   Service-Optimized QoS Flow for TCC applications

In consequence, L4S capable applications use the dedicated QoS flow of the PDU session for traffic controlled with L4S while non-L4S capable applications use the default QoS flow.

Note: The default QoS flow might still carry some non-latency sensitive traffic of L4S capable applications, for example signaling traffic to DNS servers in the connected data network.

### 2.4.1 QoS, RAN Slicing and UE Grouping Frameworks

Mobile networks must handle various traffic types, including delay-sensitive traffic, such as voice or TCC, and bandwidth demanding traffic, like high-quality video. Each traffic type has different requirements to be addressed by the RAN, the transport network, and the 5GC network.

Utilizing QoS features in the different domains is a key element to accommodate the performance requirements of the different services. They also allow prioritizing between services during times of resource shortage and contention.

The QoS Framework and available NR scheduling features provide the fundamental building blocks to configure QoS in the RAN domain.

Using the QoS Framework together with the RAN Slicing Framework feature allows configuring a network slice-specific QoS setup for different services. This is mainly required if service related QoS flows use the same 5QI value in different network slices.

When the RAN Slicing Framework feature license is enabled, the Single Network Slice Selection Assistance Information (S-NSSAI) of a network slice can be associated with a set of 5QIs and corresponding QoS profiles controlling the performance characteristics of respective DRBs in the gNodeB.

The RAN Slicing Framework feature enables the configuration of ResourcePartition and ResourcePartitionMember MO instances representing the network slices and corresponding ResourcePartition.related5qiTable references. These references point to DU5qiTable, CUUP5qiTable or CUCP5qiTable MO instances representing the QoS profiles that are used for the different service flows in a network slices.

A Managed Object Model (MOM) configuration example is shown in the figure below where service flows in different network slices share the same 5QI value (5QI 7). The service flows are differentiated for RAN scheduling by using the NR Relative Priority Scheduling feature. A higher relative priority is assigned to the service flow in the CIoT network slice. Services that require a certain minimum data rate for their operation, like for example cloud gaming, can alternatively use the NR Rate-Controlled Scheduling feature.

Note: With the Priority-Controlled Scheduling feature a different DU5qi.priorityLevel attribute value must be used to realize the network slice-specific QoS differentiation for services using the same 5QI value. This is because the PriorityDomainMapping MO instances defining the absolute scheduling priorities are shared across all DU5qi MO instances.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 12   Network Slice-Specific QoS Setup in RAN

The use of higher scheduling priorities provides services with higher throughput and better latency characteristics during times of resource contention. For example, QoS flows using the Priority-Controlled Scheduling, NR Rate-Controlled Scheduling or NR Relative Priority Scheduling features can be configured with different absolute and relative scheduling priorities to improve their performance characteristics.

However, the use of higher scheduling priorities does not provide explicit control over the latency performance to stay below a certain Packet Delay Budget (PDB) in RAN. Moreover, the use of absolute priorities always comes with the risk that service flows with higher absolute priorities (lower Priority Domain values) starve service flows with lower absolute priorities (higher Priority Domain values) in case of resource contention. Additional resource control is required to prevent that too many radio resources are consumed by flows with high absolute scheduling priorities, especially when experiencing poor radio conditions.

To control the resource consumption of service flows, either the mechanisms inbuilt to the different scheduling features or the NR Radio Resource Partitioning feature can be used. Radio resource partitions provide explicit control over the available NR cell resources irrespective of the scheduling feature being used. The use of radio resource partitions is recommended to protect service flows with low priorities from being starved. This is especially needed when using absolute priorities for scheduling and the resource control mechanisms inbuilt to scheduling features are not used or insufficient. With NR Relative-Priority Scheduling, the scheduling weight of service flows with a low relative priority increases over time dependent on the waiting time to be scheduled. This assures that service flows with low relative priorities eventually get scheduled which mitigates the starvation problem and need for resource control.

For an overview of the different NR scheduling features, see E2E QoS Guideline.

When using NR downlink carrier aggregation, the NR QoS-Aware Downlink Carrier Aggregation or NR Carrier Aggregation Scheduling Optimization for External SCells features must be enabled to support the QoS awareness and prioritized treatment of traffic sent to external SCells. The features improve the throughput and delay characteristics of external traffic competing with local traffic for SCell resources in inter-gNodeB and inter-capacity module carrier aggregation scenarios. The NR Carrier Aggregation Scheduling Optimization for External SCells feature can only optimize the traffic scheduling for external SCell partners providing low delay (RTT) on the E5 interface. The scheduling priority in the external SCells then follows the priority of the corresponding QoS flow in the PCell and the NR QoS-Aware Downlink Carrier Aggregation configuration is ignored. For SCell partners with larger E5 delays, the scheduling priority in the external SCells follows the SCell specific QoS configuration provided with the NR QoS-Aware Downlink Carrier Aggregation feature and is shared by all network slices using the same SCell.

Note: If these features are not enabled, traffic is scheduled based on the default priority domain (PD 48) in external SCells.

The UE Grouping Framework feature provides means to customize the behavior of RAN features and the RAN performance observability with Event Based Statistics for NR (EBS-N).

The feature supports the creation of UE groups based on certain selection criteria. To support a network slice or service flow specific differentiation, the following selection criteria are of main relevance for the grouping of UEs:

- PLMN
- S-NSSAI
- 5QI

For the complete list of supported selection criteria, see UE Grouping Framework.

The use of the network slice identifier (S-NSSAI) as UE group selection criteria is optional if all services share the same network slice. If TCC services are running in a dedicated network slice, the use of the QoS flow identifier (5QI) as selection criteria is optional if only a single TCC service is deployed. It might also be optional if multiple TCC services are deployed, and service-specific differentiation is not required.

The figure below shows the generic approach to customize RAN features with the UE Grouping Framework.

In the example, a RAN feature represented by Function is configured depending on the specified service flow (5QI) and network slice identifier (S-NSSAI). The differentiated configuration of the feature is represented by a corresponding profile named FunctionUeCfg. Three UE groups are created:

- CIoT UE Group Configures the RAN feature specifically for the TCC service flow with 5QI 7 in the network slice with S-NSSAI 2. Additional CIoT UE groups might be needed to differentiate multiple TCC service flows in the same network slice.
- VoNR UE Group Configures the RAN feature specifically for the VoNR service flow with 5QI 1 in the network slice with S-NSSAI 1.
- Base UE Group Configures the RAN feature for other service flows in any network slice not matching any of the previous UE groups. The Base UE group can be considered to provide the default behavior for the utilized RAN feature.

These UE groups are used in following subsections to demonstrate the differentiated configuration of the TCC toolbox features covered in this guideline.

Note: In some RAN feature configuration examples presented in later sections, the Base UE Group is shown as being associated with a specific network slice. This is mainly done for illustrative purposes if the service flows matching the Base UE group are associated with this network slice.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 13   UE Grouping Framework - Concept

The UE group selection is controlled by configuring the UeGroupSelection MO with the corresponding group definition MOs where the selection criteria for the UE group are specified.

Depending on the RAN feature, different UE configuration group types are supported which determine the use of the group definition MO. For example, mobility features per default use the mobility group (UE\_MOBILITY\_GROUP) and thus the UeMobilityGroupDefinition MO is used to configure the relevant selection criteria through the UeMobilityGroupDefinition.selectionCriteria attribute. Other RAN features, for example related to link adaptation, instead use the service group (UE\_SERVICE\_GROUP) as default UE configuration group type.

The group Id in the group definition MOs, that means the ue{Admission, Mobility, Service}GroupId attribute value, controls the selection of the UE group specific function configuration profile represented by the FunctionUeCfg MO. The profile is selected based on the ueConfGroupList attribute value that is matching the group Id.

The prioritization between different UE groups is only needed if the group evaluation selects multiple UE group instances belonging to the same UE configuration group type. This is mainly the case if UEs use multiple network slices or services in parallel.

Note: A UE can be evaluated into one instance of each UE configuration group type. No prioritization amongst UE group instances belonging to different UE configuration group types is needed.

#### 2.4.1.1 QoS-Controlled SR Scheduling

QoS-controlled SR Scheduling is part of the QoS Framework feature and allows controlling the prioritization of Scheduling Requests (SRs) for uplink traffic during high load. It helps to reduce the packet loss and latency for uplink traffic by prioritizing the SRs of Data Radio Bearers (DRBs) carrying TCC or voice service flows over DRBs carrying other service flows, like eMBB data.

Without the feature, all SRs competing for PUCCH resources are scheduled with the same priority which is higher than the priority of any UE DRB. If the feature is enabled, the initial priority for the SRs starts according to the highest priority DRB of the UE as shown in the figure below.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 14   QoS-Controlled SR Scheduling

The initial SR priority can be determined conditionally or unconditionally. Conditionally means that another DRB must be established before the priority of the DRB associated with the condition is considered. This is the case for the 5QI 5 DRB in the example above. Only while 5QI 1 is established, the initial SR priority of the UE is lifted to the priority domain value configured for 5QI 5.

Note: The SR priority starts with the highest DRB priority and increases over time. After a predefined time, it is elevated above any DRB priority to secure that the SRs of lower priority DRBs are served.

The figure below shows the corresponding MOs to configure the feature for a Critical IoT deployment example. QoS-controlled SR scheduling is enabled for all 5QIs using the Base SrHandlingUeCfg profile. The 5QI 5 is associated with the 5QI 1 related condition which is configured through the SrHandlingUeCfg.conditional5qi attribute.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 15   QoS-Controlled SR Scheduling MOM

If required, the feature can be customized for different groups of UEs by using the UE Grouping Framework feature or by referencing dedicated SrHandling MO instances for each network slice through network slice-specific DU5qiTable and DU5qi MO instances.

It is not recommended to mix the legacy SR handling with the QoS-controlled SR handling modes in the same network, for example, by disabling the feature for the CIoT network slice while enabling it for the eMBB network slice. A high number of Critical IoT UEs using the legacy SR scheduling mode can impact the SR, Buffer Status Report (BSR), retransmission and Signaling Radio Bearer (SRB) handling of the eMBB UEs in the network.

### 2.4.2 NR Radio Resource Partitioning

The NR Radio Resource Partitioning feature provides a solution to control the distribution of NR cell resources, that means the Physical Resource Blocks (PRBs) for data transmissions on PUSCH and PDSCH, across PLMNs, network slices (S-NSSAIs), and services (5QIs).

It can be used to reserve a certain minimum share of resources for services scheduled with lower priorities. This will protect them from services scheduled with higher priorities consuming all available radio resources. Reserving sufficient resources for TCC services reduces the risk for resource contention which is one of the major contributors to latency.

The figure below shows a radio resource partitioning example for three services running in two network slices. Different scheduling features and priority settings are used for the corresponding service related QoS flows.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 16   NR Radio Resource Partitioning

The gaming service flow (5QI 7) in the CIoT network slice uses the NR Rate-Controlled Scheduling feature and is configured with a prioritized rate of 10 Mbps for downlink traffic. The prioritized rate setting should be based on the minimum data rate required for gaming services to run with sufficient quality on mobile devices. The low data rate uplink traffic of the gaming service is scheduled without rate control in PD 46. This provides game control traffic with optimal scheduling performance. A prioritized rate might also be configured for uplink traffic in case many gaming users are present and a fairer uplink resource distribution across services is desired.

Depending on the given radio conditions, flows with a prioritized rate consume a varying amount of radio resources. For example, many gaming users in poor coverage can eat up almost all available radio resources in the NR cell. The performance of other service flows in the same NR cell is impacted if the configured elevation priority to sustain the prioritized data rate is higher than the priority of the other service flows. In the example the gaming service flow (5QI 7) has a higher absolute priority, that means lower Priority Domain value, than the Internet service flow (5QI 9). In consequence, the Internet service can be starved by the gaming service during resource contention. When the prioritized data rate is reached for the 5QI 7 service flow, the priority is demoted to use the same absolute priority as the 5QI 9 service flow.

To protect the Internet service in the eMBB network slice from being starved it is assigned to a radio resource partition. This provides it with a minimum share of resources during times of resource contention. The gaming service is not associated with a partition since the QoS and scheduling configuration already provides it with prioritized access to resources.

The voice service flows (5QI 1 and 5QI 5) in the eMBB network slice are configured with the highest absolute priority for scheduling and cannot be starved by the gaming service flow when there is a shortage of radio resources. Since radio resource partitions are configured for the eMBB network slice, both are additionally configured as RRP-prioritized service flows. This is achieved by configuring their 5QI values through the CellResourceMapping.priorityPartition5qiList or PartitionMapping.priorityPartition5qiList attributes provided by the NR Radio Resource Partitioning feature. This will exempt them from being partitioned and provide them always with prioritized access to radio resources during high load.

Radio resource partitions cannot protect services against the impact of RRP-prioritized traffic. Depending on the amount of prioritized service flows sufficient headroom must be left in the partition share configuration. Otherwise, the partitions cannot guarantee the allocated resource shares to service flows that are mapped to them. In the example above only 60% of the available cell resources are allocated to the partition used for the Internet service flow (5QI 9). The remaining 40% are left unallocated.

The figure below shows the corresponding MOM configuration related to the NR Radio Resource Partitioning feature.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 17   NR Radio Resource Partitioning MOM

An alternative radio resource partitioning example is shown in the figure below. Here the gaming service flow (5QI 7) is assigned to a dedicated partition and uses the Priority-Controlled Scheduling feature.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 18   NR Radio Resource Partitioning Alternative

The purpose of the partition is to assure assure the required throughput performance (minimum data rate) for the gaming service besides controlling the scheduling order of the service flows depending on their partition utilization.

The performance for the gaming service flow relies on an appropriate partition share configuration. The share must be determined based on several input criteria including the available spectrum bandwidth, estimated number of users, user activity factors, throughput during busy hour times, among others. Unless the partition of the gaming service flow is overutilized, it is scheduled with a higher priority than the Internet service flow (5QI 9) in the eMBB network slice.

The NR Radio Resource Partitioning (NR RRP) feature provides an extra level of scheduling control or service prioritization that is related to the partition utilization. Based on the resource allocation policy configuration associated with a partition, it can be decided how the service flows are prioritized against each other if the shares of their partitions are overutilized. The NR RRP-controlled scheduling prioritization can be configured through the ResourceAllocationPolicy.demotedRrpSchedulingPriority or ServicePolicy.demotedRrpSchedulingPriority attributes.

In the example above, the Internet and gaming service flows (5QI 9 and 5QI 7) are down prioritized in case their respective partition shares are overutilized during high load. Both services are then scheduled according to their 5QI-specific QoS configuration within the DEMOTED\_NORMAL priority category. The scheduling order during low load (partitions underutilized) and high load (partitions overutilized) is shown on the right side of the figure. If the Internet service flow should instead be prioritized over the gaming service flow during partition overutilization, the NR RRP scheduling priority configuration of the gaming service partition must be changed to DEMOTED\_LOW. The scheduling order with DEMOTED\_NORMAL for the gaming service partition is almost identical to a NR RRP setup where the gaming service flow is not mapped to a partition. The only difference is that the gaming service flow is then also scheduled in the DEMOTED\_NORMAL priority category if there is no resource contention. This means that the Internet service flow is prioritized during low load until it overutilizes its partition share.

Service flows that are not mapped to any partition are implicitly treated according to the NR RRP scheduling priority DEMOTED\_NORMAL. Prioritized service flows, like 5QI 1 and 5QI 5, are implicitly treated according to the highest NR RRP scheduling priority called ELEVATED.

Note: If different TCC service flows run in the same slice and are mapped to the same partition, for example a gaming service flow (5QI 7) together with an AR service flow (5QI 80), they can be prioritized against each other by using the NR Relative Priority scheduling feature and assigning different relative priorities.

The figure below shows a NR Radio Resource Partitioning example if all services share the same network slice.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 19   NR Radio Resource Partitioning with Shared Network Slice

In this case the Internet service flow (5QI 9) and the gaming service flow (5QI 7) are mapped to dedicated partitions. It is not possible to only map the Internet service flow to a partition while leaving the gaming service flow unmapped. The reason is that the creation of a PartitionMapping and ResourceAllocationPolicy MO instance for a network slice implicitly maps all its services to a partition. Individual service flows cannot be exempted from the partition mapping, except when prioritizing them through the PartitionMapping.priorityPartition5qiList attribute.

In consequence, the gaming service flow is provided with dedicated partition and radio resource share which is used by NR Rate-Controlled Scheduling to provide the service with sufficient resources to achieve the prioritized data rate. If this data rate cannot be achieved and the partition share is overutilized, the gaming service flow is down prioritized according to the given ResourceAllocationPolicy.demotedRrpSchedulingPriority and ServicePolicy.demotedRrpSchedulingPriority attribute settings.

In the example above, the gaming service flow is configured with a lower NR RRP scheduling priority than the Internet service flow. When both services overutilize their partition shares and start to compete for the remaining radio resources in the NR cell, the Internet service flow is prioritized by the scheduler and can eat up all remaining resources.

The DEMOTED\_LOW prioritization for the gaming service flow can be used to mitigate potential resource overutilization effects with the NR Rate-Controlled Scheduling feature. The resource overutilization might be caused by too many gaming users in poor radio conditions not reaching the configured prioritized rate. If their resource demand reaches the partition share limit, the gaming service flows are down prioritized by NR RRP although the prioritized rate is not achieved.

If the same NR RRP scheduling priority setting is used for both partitions, the gaming service flow is only prioritized while the prioritized rate is not achieved. Then it is scheduled with the absolute priority according to the Priority Domain value configuration for the service flow which is 46 in this example.

The figure below shows the corresponding NR Radio Resource Partitioning MOM for the shared network slice example.

Note: The allocated spectrum shares in the ResourceAllocationPolicy and ServicePolicy MOs are set to 80% and 25%, respectively. This provides the Internet service flow with an effective partition share of 60% (75% of 80%) and the gaming service flow with an effective share of 20% (25% of 80%).

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 20   NR Radio Resource Partitioning MOM with Shared Network Slice

Related Information

- E2E QoS Guideline
- NR SA Network Slicing Guideline

#### 2.4.2.1 Feature Interactions

Uplink Configured Grant

If the Uplink Configured Grant for URLLC Public RAN feature is enabled for a service flow, it has prioritized access to the reserved uplink resources and is exempted from being partitioned. With respect to the resource utilization of partitions, it is handled like an RRP-prioritized service flow whose resource utilization for uplink traffic can be observed through the corresponding pmEbsMacRBSymUtilUlPriorityPartitionDistr EBS-N counter.

This is shown in the figure below where the feature is enabled for the gaming service flow using 5QI 7.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 21   NR Radio Resource Partitioning - Uplink Configured Grant Interworking

The amount of uplink resources available to the conﬁgured radio resource partitions shrinks, depending on the number of uplink conﬁgured grant users, the grant periodicity and grant size.

Uplink traffic and corresponding resource usage is correctly partitioned when dynamic grants are scheduled to UEs configured with uplink configured grants. Dynamic grants are scheduled based on BSR or SR messages which UEs might send if the uplink traffic pattern is non-deterministic and does not match the configured grant periodicity and size.

For additional information, see  Uplink Configured Grant.

### 2.4.3 Link Adaptation

Link adaptation is one of the key technologies to deal with varying radio channel conditions. Depending on the radio channel conditions, link adaptation tries to maximize the transmission rate in relation to the transmission reliability, that means the Block Error Rate (BLER) to be maintained.

It is therefore essential to find the right link adaptation configuration in terms of BLER target and Modulation and Coding Scheme (MCS) table settings by considering relevant performance criteria, the traffic type, and the radio coverage.

3GPP standardized three MCS tables for data transmissions in the Physical Downlink Shared Channel (PDSCH) which can be used with the 5G NR supported modulations:

- 64 QAM Table (TS 38.214 - Table 5.1.3.1-1) This table is mainly used for macro cell deployments in NR
- 256 QAM Table (TS 38.214 - Table 5.1.3.1-2) This table is mainly beneficial for small cell or hotspot deployments in NR to maximize the spectral efficiency for UEs in very good radio conditions.
- Low Spectral Efficiency 64 QAM Table (TS 38.214 - Table 5.1.3.1-3) The low spectral efficiency table increases the transmission reliability for UEs in coverage areas with a low Signal to Noise Ratio (SNR) at the cost of lower throughput and higher radio resource utilization. It is mainly recommended for low data rate services requiring a higher transmission reliability in such areas. The use of this MCS table requires the activation of the Link Adaptation with Low Spectral Efficiency MCS Table for Public RAN feature.

For the TCC service deployment in the public RAN the use of the 64 QAM or 256 QAM tables is generally recommended to configure a suitable link adaptation setting considering the radio conditions, transmission rate and reliability targets in low- and mid-band NR Cells.

The BLER target for data transmissions of services must be tuned depending on the required reliability, performance, and traffic type. Real-time media TCC services are expected to perform better with BLER targets &lt;= 10%. The main goal is to avoid the retransmission of large amounts of data which impacts the service performance. TCC services like cloud AR/VR/XR or cloud gaming result in a high traffic utilization and consist of traffic types using large packet sizes. The retransmission overhead for these services can be significant, especially for UEs in poor coverage.

Link adaptation for signaling transmissions (PDCCH) is also a key factor impacting the service performance. PDCCH decoding or scheduling failures can cause a higher impact than PDSCH scheduling failures especially if Discontinuous Reception features (DRX) are enabled. If the gNodeB does not receive any UE response for downlink data transmissions indicated through PDCCH signaling, it must wait until the next occasion when UE is awake before it can retransmit the data.

Following features are available to customize the link adaptation operation on the gNodeB according to the performance needs of services:

- Link Adaptation for URLLC in Public RAN Uses lower MCS to improve transmission reliability up to 99.99% within two transmissions. By reducing the number of transmissions, 5 ms one-way latency in the RAN can be achieved. The UE Grouping Framework feature can be used to customize the feature for different UE groups. The feature is optimized for TCC services with small or medium packet sizes and only available for high band cells (FR2). The feature is part of the Critical IoT for Public RAN Base Package (FAJ 801 1700).
- NR Service-Adaptive Link Adaptation Provides a configuration framework that enables the customization of the link adaptation process for different UE groups. The UE Grouping Framework feature is required to use NR Service-Adaptive Link Adaptation. The feature is only available for low and mid band cells (FR1) and part of the Performance Boost value package (FAJ 801 4021).
- Link Adaptation for Low Target BLER for Public RAN Improves the outer loop link adaptation by allowing the target BLER to be adjusted to values lower than 1%. An initial target BLER as low as 0.1% is supported for PDSCH and PUSCH transmissions. Low target BLER settings are mainly recommended for low data rate services requiring transmission reliability. The feature adopts the configuration framework of the NR Service-Adaptive Link Adaptation feature and is available for Low and Mid Band cells (FR1). The feature can be used with any of the MCS tables mentioned above and is part of the Critical IoT for Public RAN Base Package (FAJ 801 1700).

For real-time media service deployments in FR1, mainly the NR Service-Adaptive Link Adaptation feature is needed to customize the link adaptation operation. However, the configuration of certain MO attributes, for example DlLinkAdaptation.dlSinrBackoff, DlLinkAdaptation.dlDownStep and the corresponding attributes for link adaptation in uplink direction, also requires the activation of the Link Adaptation for Low Target BLER for Public RAN feature.

The figure below shows a generic example how to use the NR Service-Adaptive Link Adaptation feature together with the UE Grouping Framework feature to realize service-differentiated link adaptation configurations in FR1.

Note: The Base instances of the profile MOs are automatically created and referenced with the shown default settings for the link adaptation and HARQ modes.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 22   Link Adaptation - UE and Service Differentiation Example

In the example, the eMBB data service (5QI 9) uses the general (default) link adaptation setting in the FR1 cell. The general setting is determined based on the LinkAdaptationUeCfg MO instance of the Base UE Group.

The IMS-based voice service (5QI 1) uses a customized link adaptation setting. The customized setting is determined based on the VoNR UE group that is selected based on respective snssai, 5qi and rrcEstResCause criteria.

Note: The customized link adaptation configuration in FR1 requires the LinkAdaptationUeCfg.linkAdaptationUeMode attribute being set to CUSTOM.

The TCC service (5QI 7) in the CIoT network slice uses another customized link adaptation setting in the FR1 cell which is determined based on the CIoT UE Group.

There is no need to customize the HARQ settings for the real-time media services in the CIoT UE group. For downlink traffic the default configuration with the HarqUeCfg.dlHarqMode attribute being set to HARQ\_NORMAL is typically sufficient and allows for up to 5 HARQ transmissions (1 initial HARQ transmission followed by 4 HARQ retransmissions). Increasing the number of HARQ transmissions by setting the attribute to HARQ\_ROBUST, which allows up to 8 transmissions, might only increase the latency performance. Moreover, any additional retransmissions might also fail if there are still errors after 4 HARQ retransmissions.

In general, the HARQ mode should be chosen dependent on the service performance requirements and be aligned with the link adaptation configuration related to the BLER target, SINR backoff and outer loop adjustment step. For example, increasing the SINR backoff results in a more conservative estimate of the UE reported channel quality and thus a more robust MCS. This reduces the likelihood for retransmissions but at the cost of achieving lower throughput.

In case the VoNR and TCC services run in parallel on the same UE, prioritization amongst the different UE groups is needed to decide upon the link adaptation setting. The chosen link adaptation setting then applies to all active service flows on the UE. In the example, the VoNR UE group is prioritized by configuring a higher value for the UeServiceGroupDefinition.ueServiceGroupPriority attribute.

Note: If only a single network slice is deployed and all services share the same network slice, the snssai selection criteria might be omitted from the UE group definitions.

If the link adaptation configuration must be differentiated across frequency bands, multiple configuration profiles are needed for the VoNR and TCC UE groups. For example, for FR1 there could be separate link adaptation configuration profiles, that means LinkAdaptationUeCfg MO and subordinate MO instances, for FDD and TDD frequencies.

The link adaptation process can be customized separately for uplink and downlink data transmissions through the attributes in the UlLinkAdaptation and DlLinkAdaptation MO classes. The most important attributes to control the reliability targets for the TCC service flows of the real-time media use case family are the UlLinkAdaptation.ulBlerTarget and DlLinkAdaptation.dlBlerTarget.

To customize the link adaptation process for signaling transmissions in the PDCCH, the PdcchLinkAdaptation.pdcchLaSinrUeOffset attribute can be used. The attribute is configured as an offset that is added to the estimated Signal to Interference and Noise Ratio (SINR) which is determined based on the Channel Quality Indicator (CQI) reported by the UE. With negative offset values, the PDCCH signaling becomes more robust but requires more resources which in turn reduces the number UEs that can be served with the available PDCCH resources.

A service-differentiated link adaptation configuration can also be realized with the Link Adaptation for URLLC in Public RAN feature in FR2 cells. In this case, the LinkAdaptationUeCfg.linkAdaptationUeMode attribute must be set to ROBUST\_BASIC or ROBUST\_HIGH. This is the only attribute in the LinkAdaptationUeCfg MO class hierarchy that can be configured; the subordinate MO classes and their attributes are not configurable with this link adaptation feature. Moreover, when robust link adaptation is configured through the LinkAdaptationUeCfg related MO classes, the NRCellDU.ulRobustLaEnabled and NRCellDU.dlRobustLaEnabled attributes must be set to false.

Some generic link adaptation configuration examples are shown in the table below. Concrete values for each MO attribute must be determined depending on the service and radio deployment.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 23   Link Adaptation Configuration - Example Table

For information about the VoNR link adaptation configuration, see the 5G Voice SA RAN Solution Guideline.

#### 2.4.3.1 Feature Interactions

NR Carrier Aggregation

When NR carrier aggregation is configured, the link adaptation settings configured through the NR Service-Adaptive Link Adaptation feature are only considered for the UEs primary cell (PCell) but not for any secondary cell (SCell). For the SCell related component carriers, link adaptation is operating according to basic settings which cannot be modified through the MOM.

This might impact the load distribution between the serving cells and the SCell activation and deactivation behavior if the Data-Aware Carrier Management Feature is enabled. The more robust the link adaptation in the PCell, the more traffic might escape to SCells providing sufficient capacity. This might violate the intended strategy to increase transmission reliability for TCC services through a customized link adaptation configuration.

To mitigate this issue, one of the following options can be considered:

1. Disabling NR Carrier Aggregation for TCC services.
2. Tuning the flow control for TCC services so that their QoS flows are scheduled on the PCell only.

The figure below shows a configuration example for the two options which require following additional feature:

- User- and Service-Specific Carrier Aggregation

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 24   Link Adaptation - NR Carrier Aggregation Interaction Options

For option 1, the User- and Service-Specific Carrier Aggregation feature allows controlling the NR carrier aggregation setup and can be used to disable and de-configure the aggregation of SCell component carriers of the UE. In the example above this is done for the VoNR and CIoT UE groups by setting the CaCellProfileUeCfg.maxDlCcAllowed and CaCellProfileUeCfg.maxUlCcAllowed attributes to 1.

The performance of other service flows, like 5QI 9, is impacted if NR carrier aggregation is de-configured. NR carrier aggregation is reconfigured after the VoNR or Critical IoT services terminate, and the corresponding CaCellProfileUeCfg attribute settings of the Base UE group are re-evaluated.

In addition, the SCell frequency usage with NR carrier aggregation can be controlled. In case TCC services should be restricted to certain frequencies, the disallowed frequencies can be configured through the CaCellProfileUeCfg.blockedSCellFreqList attribute.

For option 2, the User- and Service-Specific Carrier Aggregation feature allows controlling the scheduling for each service flow (5QI) when NR carrier aggregation is established. In the example above, the traffic of the VoNR and TCC service flows is scheduled to the PCell only by configuring the SchedulingProfile.dlCaSchedulingMode attribute. The NR carrier aggregation related scheduling control is only possible for downlink traffic but not for uplink traffic with NR uplink carrier aggregation.

The benefit with this approach is that the 5QI 9 service flow is not impacted by the scheduling decisions taken for the other service flows. The 5QI 9 traffic is still scheduled across all serving cells. The UE Grouping Framework feature is also not required for option 2 since the differentiation is realized for each 5QI through the default DU5qiTable and corresponding DU5qi child MO instances.

Note: If service flows with overlapping 5QI values are used across network slices, the same differentiation can be realized by using network slice-specific DU5qiTable and DU5qi MO instances.

NR-NR Dual Connectivity

With NR-DC it is currently not possible to configure a uniform link adaptation strategy for all serving cells of the UE since different link adaptation features are supported for different frequency ranges, FR1 and FR2.

The use of different link adaptation strategies in FR1 and FR2 might influence the traffic distribution across Master Cell Group (MCG) and Secondary Cell Group (SCG) resources for Secondary Node terminated split DRBs.

Note: It might be possible to tune the link adaptation settings in FR1 with the NR Service-Adaptive Link Adaptation feature as close as possible towards the robust settings realized with the Link Adaptation for URLLC in Public RAN feature in FR2.

Moreover, when service flows of Secondary Node terminated split DRBs are aggregated and traffic is sent simultaneously over both MCG and SCG resources, the packet reordering in the UEs Packet Data Convergence Protocol (PDCP) layer can increase the throughput and latency variations and negatively impact the service performance. In short, PDCP aggregation may increase latency due to packet reordering or packet loss on Xn-U transport.

To mitigate these issues, one of the following options might be considered:

1. Disabling PDCP Aggregation and scheduling TCC service flows either on the MCG or SCG.
2. Disabling NR-DC while TCC service flows are active on the UE
3. Disabling the Secondary Node termination for TCC services. The respective QoS flows are always Master Node terminated and scheduled on the MCG only.

By using one of these options, the transmission robustness is solely based on a single link adaptation strategy, either the one configured for FR1 cells on the Master Node or the one configured for FR2 cells on the Secondary Node.

For option 1, following additional features are used on the Secondary Node to customize the PDCP aggregation and flow control behavior:

- LTE-NR Downlink Aggregation (required)
- NR Service-Adaptive User Plane Profile (required)
- Service-Adaptive Uplink User Plane Profile (required)
- LTE-NR Uplink Aggregation (optional)

The figure below shows the relevant NR-DC configuration for option 1.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 25   Link Adaptation - NR-DC Interaction Option 1

With option 1, the PDCP aggregation and MCG or SCG resource usage for Secondary Node split DRB traffic in downlink direction can be controlled separately for each service flow. In the example, downlink PDCP aggregation is disabled for the TCC service flow (5QI 7) in the CIoT network slice by setting the DcDlCfg.dcDlAggAllowed attribute to False. Additionally, the downlink cell group switching is disabled through the CgSwitchCfg.dlCgSwitchMode attribute. Per default, the TCC service traffic is then scheduled over the SCG if the PSCell quality is good. Otherwise, it is scheduled over the MCG. To change the cell group to the MCG, the DcDlCfg.dlPdcpPreferredCgOverride attribute must be set to MCG. A similar behavior can be achieved by leaving the downlink cell group switching enabled and setting the DcDlCfg.dlPdcpPreferredCgOverride attribute to either KEEP\_SCG or MCG. For the eMBB data service flow (5QI 9) downlink PDCP aggregation and cell group switching are allowed.

Note: The default CUUP5qiTable MO instance is used control the downlink PDCP aggregation for each service flow, that means no network slice-specific QoS setup is required.

In uplink direction, the PDCP aggregation and initial MCG or SCG use can be controlled for each QoS flow based on the 5QI value. By setting the UlDataSplitUeCfg.initialUlPrimaryPathMode attribute to SCG, the UE starts sending uplink data on the SCG once the Secondary Node terminated split DRB is established. Uplink PDCP aggregation is disabled for the TCC service flow (5QI 7) by setting the UlDataSplitUeCfg.ulDataSplitThreshScg and UlDataSplitUeCfg.ulDataSplitThreshScgLow attributes to -1.

Note: The UlDataSplitUeCfg.ulDataSplitThreshScg and UlDataSplitUeCfg.ulDataSplitThreshScgLow attributes must only be configured if the LTE-NR Uplink Aggregation feature is enabled. If the feature is disabled, no uplink PDCP aggregation is performed.

Uplink switching between the MCG and the SCG can be controlled through the UlCgSwitchCfg.ulCgSwitchMode attribute.

Cell group switching for uplink and downlink traffic can be disabled for stationary UEs assuming that they always have good FR2 coverage. If good FR2 coverage cannot be assured, cell group switching should be enabled to allow MCG usage when the UE moves into poor FR2 coverage areas. If cell group switching is disabled, radio link failures due to poor coverage trigger the release of the Secondary Node which causes service interruptions.

Note: Frequent switching between MCG and SCG resources also impacts the throughput and latency performance of services.

The figure below shows the NR-DC configuration for options 2 and 3.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 26   Link Adaptation - NR-DC Interaction Options 2 and 3

With option 2, the NR-DC setup is disabled for the TCC service flows through the CIoT UE Group. For the eMBB data and VoNR service flows the NR-DC setup is allowed through the common Base UE group. If all service flows run on the same UE the NR-DC setup is disabled for all service flows while the TCC service flow is active.

Note: If NR-DC was already established for a service flow in the eMBB network slice, the change of the NrdcMnCellProfileUeCfg.nrdcEnabled attribute value from True to False due to a UE group re-evaluation causes the Secondary Node to be released. This might lead to a throughput degradation of the affected service flows.

If NR-DC is enabled, the termination of service flows can be controlled separately for each 5QI through the corresponding NrdcSnTermination MO instance. Option 3 in the figure above shows the relevant MOM configuration to control the Secondary Node termination for different service flows. Only the data service flow (5QI 9) in the eMBB network slice is allowed to terminate on the Secondary Node by setting the bit representing the Allocation and Retention Priority (ARP) value of this service flow to 1 in the NrdcSnTerminationUeCfg.nrdcSnTermAllowed attribute. All other service flows are terminated on the Master Node and use MCG resources.

To disallow the Secondary Node termination, the VoNR service flows (5QI 1 and 5QI 5) implicitly refer to the system provided NrdcSnTermination MO instance called SnTerminationProhibited. No extra configuration is required to achieve this. The TCC service flow (5QI 7) refers to the same SnTerminationProhibited instance by configuring the corresponding reference through the CUCP5qi.nrdcSnTerminationRef attribute.

Note: The termination of service or QoS flows of the same PDU session must be identical. It is not possible to terminate one PDU session QoS flow on the Master Node and another one on the Secondary Node. Different PDU sessions are required to terminate QoS flows differently.

For additional information, see NR-NR Dual Connectivity.

### 2.4.4 Uplink Scheduling

Three different scheduling schemes for uplink data transmissions can be realized in NR RAN:

- Dynamic grant scheduling.
- Configured grant scheduling.
- Hybrid scheduling based on dynamic and configured grants.

#### 2.4.4.1 Dynamic Grant Scheduling

UEs request resources for uplink data transmissions by using Scheduling Request (SR) and Buffer Status Report (BSR) messages. Based on these messages the gNodeB grants uplink resources to the UEs by considering the priorities of their active QoS flows which additionally contributes to the uplink latency in RAN. The figure below shows the dynamic grant scheduling scheme.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 27   Dynamic Grant Scheduling

The available Physical Uplink Control Channel (PUCCH) resources for SR messages are distributed across the UEs connected to a NR cell. UEs cannot immediately send SRs when they have uplink data. Instead, during RRC connection establishment, they are configured with predefined intervals when they are allowed to send SRs. The interval is also called SR periodicity.

If the NR Service-Adaptive SR Periodicity feature is not activated, three pre-defined SR periodicity pools are available to configure the UEs with a short, medium, or long SR periodicity. The SR periodicity and number of resources allocated to the three pools is fixed and depends on the duplexing scheme (FDD or TDD):

- Short Periodicity Pool
    - FDD: 4 slots (4 ms)
    - TDD: 10 slots (5 ms)
- Medium Periodicity Pool
    - FDD: 20 slots (20 ms)
    - TDD: 40 slots (20 ms)
- Long Periodicity Pool
    - FDD: 40 slots (40 ms)
    - TDD: 160 slots (80 ms)

At RRC establishment, the UEs are assigned to a SR periodicity pool based on a first come first served principle without any service-awareness. The first UEs connecting to the RAN are allocated to the short SR periodicity pool. Once the allocated resources of this pool are used up, the following UEs are allocated to the medium SR periodicity pool. Once the medium pool resources are depleted, any following UE is allocated to the long SR periodicity pool. In the worst case, a UE assigned to the long SR periodicity pool must wait up to 40 slots (40ms) on FDD cells or 160 slots (80ms) on TDD cells which significantly increases the uplink latency of service traffic flows.

If the NR Service-Adaptive SR Periodicity feature is activated, different SR periodicities can be assigned to UEs by using the UE grouping framework. The available SR periodicities are organized along 5 configurable SR periodicity pools.

Note: On top of these configurable pools, there is also a long SR periodicity pool which is not configurable.

The SR periodicity pool must be selected and configured according to the required latency performance of the individual service traffic flows and the anticipated number of UEs that must be served with the SR periodicity. The pool is selected through the UE group, for example by using the 5QI value identifying a certain service traffic flow. The UE group evaluation and assignment of the SR periodicity pool to the UE is triggered by Initial Context Setup, PDU Session Resource Setup, PDU Session Modification, and other messages. The signaling of the corresponding SR periodicity to the UE is done through RRC reconfiguration messages. All currently active service traffic flows on the UE will use the signaled SR periodicity for their uplink transmissions.

If the resources of the selected SR periodicity pool are depleted, the UE is assigned to the next higher SR periodicity pool for which resources have been configured through the MOM. Otherwise, it is assigned to the long periodicity pool. If all SR resources in the NR cell are depleted, the RRC connection setup, resume, or reestablishment request of the UE is rejected. If the UE is already connected and a dynamic UE group change triggers the reconfiguration of a new SR periodicity according to the selected pool, the reconfiguration is not executed if all SR resources in the NR cell are depleted. Instead, the UE keeps its current SR periodicity.

In Low and Mid Band NR cells, SRs are carried in PUCCH format 1 messages (PF1). Four Physical Resource Blocks (PRBs), two PRB pairs located at the lower and upper band edges, are available for PF1 messages with 18 resources for each PRB. The total number of PF1 resources is therefore 72. 16 of these resources are reserved for carrying HARQ ACK and HARQ NACK messages in the PUCCH. The remaining 56 resources are available for the configuration of the 5 SR periodicity pools.

Note: For NR cells supporting High Speed UEs (HSPE), only 12 resources are available for each PF1 PRB. Therefore, only 32 resources are available for the SR pool configuration.

The number of UEs that can be served with one of these SR resources depends on the SR periodicity and duplexing scheme, FDD or TDD. With FDD each slot provides an opportunity for uplink transmissions. TDD provides less opportunities since the number of available uplink slots depends on the TDD pattern as shown in the figure below.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 28   SR Resource Sharing with TDD Pattern 2 and 3

For example, to provide UEs with a SR periodicity of 5 ms (10 slots), two UEs can be served with a single SR resource for TDD pattern 2 and 3 UEs can be served with TDD pattern 3.

The total number of UEs that can be served with the 5 ms SR periodicity pool in a NR cell is determined by multiplying this number with the available SR resources in FR1. This means, 112 (2 x 56) UEs can be served with a 5 ms SR periodicity with TDD pattern 2 while 168 (3 x 56) UEs can be served with TDD pattern 3. The larger the SR periodicity, the more UEs can be served in the NR cell. For example, doubling the SR periodicity from 5 ms to 10 ms allows 224 UEs being served with TDD pattern 2 and 336 UEs with TDD pattern 3. The SR periodicity pool configuration therefore impacts the total number of UEs that can be connected in a cell.

The following table summarizes the number of UEs that can be served with a single SR resource in FR1 depending on the SR periodicity, duplexing scheme, and TDD pattern.

| SR Periodicity   | FDD   | TDD-0 (DDSU)   | TDD-1 (DDDSUUDDDD), TDD-2 (DDDSU), TDD-4 (DDDDDDDSUU)   | TDD-3 (DDDSUDDSUU)   |
|------------------|-------|----------------|---------------------------------------------------------|----------------------|
| sl4              | 4     | 1              | n/a                                                     | n/a                  |
| sl8              | 8     | 2              | n/a                                                     | n/a                  |
| sl10             | n/a   | n/a            | 2                                                       | 3                    |
| sl16             | 16    | 4              | n/a                                                     | n/a                  |
| sl20             | 20    | 5              | 4                                                       | 6                    |
| sl40             | 40    | 10             | 8                                                       | 12                   |
| sl80             | n/a   | 20             | 16                                                      | 24                   |
| sl160            | n/a   | 40             | 32                                                      | 48                   |

With the numbers in the table above and the amount of SR resources allocated to the configurable SR periodicity pools, the number of UEs for each pool can be determined. The following table shows a SR periodicity pool configuration example for a NR cell using the TDD-1 pattern DDDSUUDDDD.

| Pool   | SR Periodicity (TDD-1)   |   Number of UEs per SR Resource |   Number of SR Resources per Pool |   Number of UEs per Pool |
|--------|--------------------------|---------------------------------|-----------------------------------|--------------------------|
| Pool 1 | 10 slots (5 ms)          |                               2 |                                20 |                       40 |
| Pool 2 | 20 slots (10 ms)         |                               4 |                                 0 |                        0 |
| Pool 3 | 40 slots (20 ms)         |                               8 |                                 0 |                        0 |
| Pool 4 | 80 slots (40 ms)         |                              16 |                                36 |                      576 |
| Pool 5 | 160 slots (80 ms)        |                              32 |                                 0 |                        0 |

In this configuration example, the 56 SR resources are distributed across two SR periodicity pools:

- Pool 1 is configured for latency-sensitive TCC services with a 5 ms SR periodicity and 20 SR resources. The SR periodicity and number of SR resources are configured by setting the SrPeriodicityPoolSet.srPeriodicityPool1 attribute to SR\_PERIODICITY\_SL10 and the SrPeriodicityPoolSet.noOfSrResourcesPool1 attribute to 20.
- Pool 4 is configured with a longer SR periodicity (40 ms) using the remaining SR resources. The pool can be used for service flows that do not have stringent uplink latency requirements, for example, eMBB data services. The SrPeriodicityPoolSet.srPeriodicityPool4 attribute is set to SR\_PERIODICITY\_SL80 and the SrPeriodicityPoolSet.noOfSrResroucesPool4 attribute is set to 36.

The pools, periodicity, and number of resources for each pool must be configured according to the uplink latency requirements of the service traffic flows and anticipated number of UEs in the NR cell.

Note: To accommodate VoNR services, an additional UE group and pool, for example SrPeriodicityPoolSet.srPeriodicityPool3, must be configured. This pool also provides a SR periodicity of 40 ms and must be configured with the required number of SR resources for VoNR users. The 56 SR resources must be distributed across the pools based on the anticipated ratio of TCC, eMBB and VoNR service users in the NR cell.

For additional information about the VoNR related SR periodicity handling, see 5G Voice SA RAN Solution Guideline.

The figure below shows the corresponding MOM for this SR periodicity pool configuration example.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 29   NR Service-Adaptive SR Periodicity - MOM

When the resources for the configurable SR periodicity pools are exhausted, new UEs are assigned to the non-configurable long periodicity pool which has a slot length of 160 (sl160). Two additional PRB pairs are dynamically added for this pool providing 36 resources (24 for NR cells supporting HSPUEs). If the resources that already have been reserved for the HARQ ACK and NACK messaging in the NR cell are not sufficient, the number of resources available for SRs is reduced to 20 (8 for NR cells supporting HSPUEs). Otherwise, 1152 additional UEs (36 x 32) can be served by the long periodicity pool in a NR cell, for example, with TDD pattern 1 or 1728 UEs (36 x 48) with TDD pattern 3.

For more information, see NR Service-Adaptive SR Periodicity.

The following figure shows an example of the e2e one-way latency for uplink traffic when using different SR periodicities in a NR cell during low load. The NR cell is configured with the n78 frequency band, a carrier bandwidth of 100 Mhz, and TDD pattern DDDSUUDDDD (special slot pattern 3:8:3). The uplink traffic pattern is 50 Kbps (UDP) with a packet size of 140 bytes simulating a cloud gaming service.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 30   E2e One-Way UL Latency with different SR Periodicities

The green curve shows the e2e one-way uplink latency when using a SR periodicity of 20 ms. The pink curve shows one-way latency improvement when using a SR periodicity of 5 ms.

Note: Besides customizing the SR periodicities with the NR Service-Adaptive SR Periodicity feature, the effective e2e uplink one-way latency depends on other factors such as the service traffic pattern and cell load.

Moreover, if the SR grant is too small to empty the UE buffer, the UE must request additional grants by providing BSRs with the data sent in the PUSCH. The delay increases the more grants are needed to transmit a packet over the air interface to the gNodeB. The NR Adaptive Uplink Grant Size feature allows customizing the SR grant handling, for example by configuring TCC services with a certain grant size through the BsrUeCfg.srGrantSize attribute.

Note: For real-time media TCC services deployed over the top of mobile networks, the traffic pattern is non-deterministic. Therefore, the configured grant size value is never a perfect fit.

#### 2.4.4.2 Configured Grant Scheduling

Configured grant scheduling allows the gNodeB to reserve resources for the uplink transmission of UEs. The periodicity and amount of resources in the uplink slot is determined through configuration. The UEs can directly utilize the dedicated resources without sending SRs and waiting for the subsequent grants from the gNodeB. Therefore, the configured grant scheduling scheme can reduce the uplink latency and the PDCCH resource usage.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 31   Uplink Configured Grant Scheduling

3GPP has standardized two grant types for this scheduling scheme in TS 38.321:

- Configured Grant Type 1 The uplink grant is provided through RRC signaling and stored as configured uplink grant.
- Configured Grant Type 2 The uplink grant is provided through physical layer signaling (PDCCH). It is stored or cleared depending on the uplink grant activation or deactivation indicated through the physical layer signaling.

#### 2.4.4.3 Hybrid Scheduling

Hybrid scheduling is based on a combination of the configured grant and dynamic grant scheduling schemes. Configured grant resources are provided to UEs in uplink slots according to the configured grant periodicity and grant size. The UEs use the reserved resources to transmit initial data waiting for uplink transmission. If the grant is too small to empty the UE buffer, a BSR is included in the PUSCH to request dynamic grants for the remaining data.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 32   Hybrid Scheduling based on Dynamic Grant and Configured Grant

The Uplink Configured Grant for URLLC Public RAN feature realizes the hybrid scheduling scheme and is supported for Low Band and Mid Band frequencies in FR1. The configured grants are provided to UEs based on the 3GPP standardized configured grant type 2. The feature can be used to reduce uplink latency in RAN by customizing the configured grant periodicity and grant size depending on the TCC service being deployed. The service-specific configuration of the grant profile is supported by means of the UE Grouping Framework feature.

Each UE can be provided with a single configured grant profile. The profile is configured through the ConfiguredGrantUeCfg MO. For UEs supporting the lch-ToConfiguredGrantMapping-r16 capability, the mapping of the configured grant to logical channels can be restricted to those used for TCC traffic. For example, the mapping of the configured grant to logical channels can be disallowed for non-TCC DRB and SRB transmissions. The mapping is controlled by configuring the ConfiguredGrantUeCfg.notAllowed5qiList and ConfiguredGrantUeCfg.configuredGrantSrbConfig attributes. If no mapping or restriction is configured, the UE can use the granted uplink resources for any uplink QoS flow or signaling transmission. The mapping is configured for each DRB and SRB through the allowedCG-List-r16 attribute in the LogicalChannelConfig IE which is communicated to the UE in RRC messages.

The configured grant periodicity can either be configured on NR cell level or UE group level, dependent on how the GNBDUFunction.configuredGrantConfMode attribute is set. If the periodicity is configured on NR cell level, all UEs in the cell share the same periodicity that is configured through the NRCellDU.configuredGrantPeriodicity attribute. If the periodicity is configured on UE group level, different periodicities can be allocated to UEs in the cell according to their needs. The periodicity on UE group level is configured through the ConfiguredGrantUeCfg.configuredGrantPeriodicity attribute.

For the TCC services of the real time media use case family, the grant size should be set to zero. This is the recommended configuration for any TCC service using a non-deterministic and bursty traffic pattern. The grant periodicity can be customized based on the service and capacity needs also considering the TDD pattern in use.

Note: The grant periodicity influences the number of UEs that can be activated with uplink configured grants.

When setting the grant size to zero, a minimum number of resources is reserved and mainly used by the UEs for BSR transmissions to trigger dynamic grant scheduling. The scheduling priority and size of the dynamic grants depends on the priority and the link adaptation strategy configured for the service flow.

Note: Link adaptation is currently not operating when transmitting uplink data through the configured uplink grant resources. A fixed and robust MCS is used for those transmissions.

The UE skips uplink transmissions for the granted resources when it has no data to send.

For more information about the feature, see Ericsson Critical IoT for Public RAN All-In-One User Guide in the Ericsson Critical IoT for Public RAN CPI library.

The figure below shows an example how the Uplink Configured Grant for URLLC Public RAN feature can be used together with the UE Grouping Framework feature to configure a service-differentiated scheduling setup.

Note: The UeBbProfileUeCfg.configuredGrantUeCfgRef attribute implicitly refers to the Base instance of the ConfiguredGrantUeCfg MO if not configured.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 33   Uplink Configured Grant - UE and Service Differentiation Example

In the example, the uplink configured grant is activated for the CIoT UE Group, that means for the TCC service flow with 5QI 7, by setting the ConfiguredGrantUeCfg.ConfiguredGrantMode attribute to SINGLE.

The configured grant can only be used by the DRB carrying the 5QI 7 flow. Neither the SRBs nor the DRBs for VoNR signaling and eMBB data traffic can use it, since the ConfiguredGrantUeCfg.configuredGrantSrbConfig attribute is set to NOT\_ALLOWED and the ConfiguredGrantUeCfg.notAllowed5qiList attribute list contains the corresponding VoNR and eMBB related 5QI values.

When non-TCC service flows are active, the uplink configured grant is disabled by setting the attribute to DEACTIVATED through the corresponding ConfiguredGrantUeCfg MO instance that is selected by the UE group of the matching service.

Assuming that the VoNR service is more important than the TCC service, the VoNR UE group is configured with a higher priority compared to the CIoT UE Group. In consequence, when the VoNR service flow with 5QI 1 is set up while the uplink configured grant is active for the TCC service, the uplink configured grant is de-configured. When the VoNR service flow terminates, the uplink configured grant is reconfigured for the UE.

#### 2.4.4.4 Feature Interactions

This section mainly focuses on the interaction of the Uplink Configured Grant for URLLC Public RAN feature with other NR RAN features.

NR Service-Adaptive SR Periodicity

If the uplink configured grant feature is activated, the UE is configured with both, a SR periodicity, and a configured grant providing uplink resources with a certain periodicity. Depending on which of both opportunities for uplink transmissions is first available when the UE wants to send data, either the configured grant is used, or a SR is sent. If the uplink grant opportunity comes first, no SR is sent and vice versa.

It is therefore recommended to activate the NR Service-Adaptive SR Periodicity feature to align the SR periodicity with the configured grant periodicity. UEs with uplink configured grants should be configured with a SR periodicity that is larger than the configured grant periodicity. This helps to prevent that the UEs send SRs to request uplink resources instead of using the configured grants. If the NR Service-Adaptive SR Periodicity feature is not used, the gNodeB might configure the UE with a SR periodicity which is shorter than the configured grant periodicity if it is one of the first UEs connecting to the RAN in the NR cell.

NR Carrier Aggregation

The Uplink Configured Grant for URLLC Public RAN feature can be used together with the NR Uplink and NR DL Carrier Aggregation features. In NR carrier aggregation scenarios, the configured grant resources for uplink transmissions are always taken from the PCell to which the UE is connected. SCell resources are not affected and only used for uplink transmissions when the configured grant UE is requesting additional resources through BSRs. In consequence, sufficient PCell resources must be available to handle the uplink traffic demand of TCC service flows with configured grants in Public RAN. The use of NR carrier aggregation does not influence the number of uplink grant UEs that can be served in a NR cell.

Connected Mode DRX

Connected Mode Discontinuous Reception (DRX) and uplink configured grant are conflicting features. If the UE is in DRX sleep state while an uplink configured grant is scheduled, the allocated resources cannot be used by the UE. This will result in resource utilization inefficiencies and potential delay for data transmissions.

The Connected Mode DRX feature should be disabled for QoS flows using uplink configured grant to ensure a timely uplink communication. The NR Service-Adaptive DRX feature supports the configuration of different DRX profiles for specific UE groups. The NRCellDU.drxProfileEnabled attribute is used to enable the configuration of different DRX profiles for each NR cell through the DrxProfile and UE group specific DrxProfileUeCfg MO instances that are referenced by the NRCellDU.drxProfileRef attribute.

The figure below extends the uplink configured grant configuration example with different DRX profiles for each UE group. DRX is only enabled for the eMBB service flow with 5QI 9 while it is disabled for the TCC and VoNR service flows.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 34   Uplink Configured Grant - DRX Configuration

NR-NR Dual Connectivity

Since the Uplink Configured Grant for URLLC Public RAN feature is supported in FR1 only, the DRBs using uplink configured grants must be configured as Master Node terminated MCG DRBs when NR-DC is enabled for UEs.

The termination of QoS flows and respective DRBs can be controlled through the NrdcSnTerminationUeCfg.nrdcSnTermAllowed attribute. An example configuration to prohibit the termination of QoS flows on Secondary Nodes is shown in  Link Adaptation for the NR-DC feature interaction. The same configuration can be applied with the uplink configured grant feature.

Link Adaptation

Link adaptation is not operational for the periodic scheduling of configured uplink grants. The MCS signaled to the UE for uplink data is fixed to a robust transmission scheme.

If dynamic grant scheduling is triggered by BSRs, link adaptation is operational and works according to the settings specified through the NR Service-Adaptive Link Adaptation feature in FR1 NR cells.

NR Radio Resource Partitioning

The uplink resources that are reserved through the Uplink Configured Grant for URLLC Public RAN feature are treated as prioritized resources and are exempted from being partitioned.

For additional information, see  NR Radio Resource Partitioning.

### 2.4.5 Flow and Congestion Control

Flow control is a technique to manage the pace at which data is transmitted between two network entities. It regulates the transmission rate between them to prevent that the sender overwhelms the receiver with data. With flow control, the sender adjusts the data rate according to the receiver's capability related to buffering and processing the received data.

Congestion control is a related technique but serves a different purpose. While flow control prevents overwhelming the receiver with data, congestion control prevents overwhelming the network between the sender and receiver. Moreover, congestion control is performed e2e by senders and receivers and additionally by individual network entities in between them. The network entities between senders and receivers support the e2e congestion control operation, for example, by dropping packets or providing them with congestion feedback. The Explicit Congestion Notification (ECN) mechanism is an extension to the IP and transport protocol suite that allows the network entities to notify senders of congestion situations in the network before dropping packets. ECN is also an integral part of the Low Latency, Low Loss, Scalable Throughput (L4S) congestion control mechanism.

Flow and congestion control serve following purposes:

- Preventing buffer overflows in network elements leading to data loss.
- Adapting to changing network conditions by adjusting the transmission rate to the available bandwidth or congestions levels.
- Reducing the need for retransmissions, which is costly in terms resource usage and service performance.
- Optimizing the performance of the network.

E2e flow and congestion control mechanisms are typically implemented at the transport layer in the Open Systems Interconnection (OSI) model, for example in protocols like TCP or QUIC. They can also be present at other layers, for example in application layer protocols like the Real-Time Transport Protocol (RTP) and the RTP Control Protocol (RTCP) which are used by many real-time media applications to transmit video and audio data. RTP is typically based on UDP and does not provide any e2e flow or congestion control mechanism. However, when used in conjunction with RTCP, the performance of the media streams can be monitored to provide feedback to the application. Based on the performance feedback, the application can adapt the audio or video encoding and media streaming bitrates accordingly.

As shown in the figure below, the network between senders and receivers might also implement flow and congestion control techniques. Their support and operation might be different across network domains.

If services make use of e2e flow and congestion control techniques, the flow and congestion control operation in the different network domains must be configured to interwork with the former to provide the services with optimal performance.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 35   Flow and Congestion Control Interworking

#### 2.4.5.1 NR RAN Flow and Congestion Control

In NR RAN, flow control and congestion control are provided by the Radio-Link Control (RLC) and Packet Data Convergence Protocol (PDCP) layers of the radio protocol stack. The figure below shows some of the relevant MOs and attributes to control their operation.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 36   Flow and Congestion Control in NR RAN

Flow Control

Flow control is implemented in the PDCP layer of the gNodeB. PDCP flow control is mainly important for downlink traffic in dual connectivity scenarios using PDCP Aggregation. In contrast to the main ambition of flow control, meaning to adapt the sending data rate based on receiver feedback to prevent it being overwhelmed with data, PDCP flow control has a different target.

PDCP flow control tries to handle variations in the capacity and delay of transmissions on Secondary Node terminated split DRBs by:

- Deciding when to activate, suspend, or deactivate downlink PDCP aggregation. Downlink PDCP aggregation is activated when the capacity of one cell group is not enough for an ongoing data transmission and the time for packets waiting in the PDCP buffer crosses a certain threshold. Without downlink PDCP aggregation, the cell group providing the shorter delay is preferred per default.
- Balancing the rate and delay for the transmissions using MCG and SCG resources when downlink PDCP aggregation is active. The rates are coordinated in a way to maintain a certain target time for the buffering of packets in the RLC layers of the Master and Secondary Nodes. Reducing the difference in the RLC SDU buffering time between both cell groups minimizes the risk for delivering PDCP PDUs out of order towards the UE and therefore the experienced latency due to PDCP reordering. This is especially important in scenarios where the radio and load conditions rapidly change.

The figure below shows the PDCP flow control concept and some of the MO attributes to customize its operation.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 37   PDCP Flow Control for NR-DC using Downlink PDCP Aggregation

PDCP flow control is realized by using feedback information from the MAC and RLC layers on the Master and Secondary Nodes. The feedback is communicated in Downlink Data Delivery Status (DDDS) messages to the PDCP layer on the Secondary Node. For example, the RLC layers provide feedback about the RLC SDUs that are transmitted to the UE and acknowledged by the UE. The PDCP layer uses the corresponding time stamps to estimate their buffering time in the RLC SDU queue. This time is also called packet age. Based on the packet age, the PDCP layer adjusts the transmission rate for the MCG and SCG to reach a certain target time in which the corresponding RLC layer is able empty its buffer. The target time for each cell group is configured through the UserPlaneProfileUeCfg.dlPdcpMcgSpsTargetTime and UserPlaneProfileUeCfg.dlPdcpScgSpsTargetTime attributes.

If the packet age difference across both cell groups crosses a certain threshold, the PDCP layer suspends downlink PDCP aggregation to reduce the risk of packet reordering timeouts in the PDCP layer of the UE. The threshold is configurable through the UserPlaneProfileUeCfg.dcDlAggAgeDiffThresh attribute. Moreover, if the PDCP SDU buffer is empty for a certain time, the PDCP layer can deactivate downlink PDCP aggregation. The corresponding timer is configurable through the UserPlaneProfileUeCfg.dcDlAggExpiryTimer attribute.

PDCP flow control is enabled by activating the LTE - NR Downlink Aggregation feature. When the feature is active, the PDCP flow control settings can be configured through the UE group-adaptive UserPlaneProfileUeCfg MO class.

As dual connectivity scenarios using downlink PDCP aggregation are challenging to handle, PDCP flow control is not further addressed in this version of the solution guideline, assuming that dual connectivity is either disabled for TCC services or restricted to use either MCG or SCG resources without PDCP aggregation.

Congestion Control - L4S

Low Latency, Low Loss, and Scalable Throughput (L4S) is an e2e congestion control mechanism which addresses the latency impact related to queueing delays. Queuing delays are typically caused by congestion control algorithms that try to maximize throughput across the transmission network. In contrast, L4S pursues a strategy to minimize the queueing delays. The goal is to improve the latency performance of applications at the potential cost of reducing their throughput performance. In short, L4S balances latency performance against throughput performance.

L4S is only beneficial for applications that can adjust the sending data rates based on the congestion information signaled by the network. L4S capable network elements monitor the queueing time or packet age on bottleneck links and signal corresponding congestion information to the applications. The applications in turn adapt their sending data rates, for example by changing media codecs, based on the amount of congestion information they receive. By influencing the sending data rate, L4S reduces the queueing delay and packet loss probability.

The high-level concept for congestion control with L4S support on the gNodeB is shown in the figure below.

Note: The gNodeB does not support L4S for QoS flows using PDCP Robust Header Compression.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 38   4S support in NR RAN

The Uplink RAN-Assisted Rate Adaptation with L4S for Public RAN and Downlink RAN-Assisted Rate Adaptation with L4S for Public RAN features enable the L4S support in the gNodeB. The features monitor the queueing delay of packets and provide L4S congestion feedback to the application endpoints (client side) by marking the IP packets based on a probability which is determined from the configured L4sCfg.ulL4sLowThresh, L4sCfg.dlL4sLowThresh, L4sCfg.ulL4sHighThresh and L4sCfg.dlL4sHighThresh attribute values. If the queueing delay for uplink or downlink traffic is lower than the L4sCfg.ulL4sLowThresh or L4sCfg.dlL4sLowThresh attribute value, the congestion marking probability is 0% and no marking is done. If the queueing delay is larger than the L4sCfg.ulL4sHighThresh or L4sCfg.dlL4sHighThresh attribute value, the congestion marking probability becomes 100% and every packet is marked by setting the ECN Congestion Experienced (CE) bits in the IP header. If the queueing delay is between these thresholds, certain IP packets are marked based on the calculated probability.

The threshold configuration should be based on the one-way latency and throughput requirements of the rate-adaptive service flows:

- Low threshold settings, for example ulL4sLowThresh or dlL4sLowThresh = 4 ms and ulL4sHighThresh or dlL4sHighThresh = 14 ms, are suitable for service flows with strict latency requirements but risk to reduce their overall throughput.
- Default threshold settings, ulL4sLowThresh or dlL4sLowThresh = 8 ms and ulL4sHighThresh or dlL4sHighThresh = 20 ms, are suitable to provide service flows with a balanced throughput and latency performance.
- High threshold settings, for example ulL4sLowThresh or dlL4sLowThresh = 12 ms and ulL4sHighThresh or dlL4sHighThresh = 40 ms, are suitable for service flows with relaxed latency requirements also causing less reduction of their overall throughput.

For the configuration of the L4S thresholds, the traffic profile of the rate adaptive services must be considered. Low marking thresholds are recommended for services like video conferencing which inject large-sized packets at low data rates. Low marking thresholds assure that the packet marking ratio is high enough for L4S to balance the throughput and latency performance in an optimal way. With high marking thresholds small packet sizes are needed to provide enough packets and increase the marking ratio when the data rate is low.

Note: Configuring the low and high threshold values very close to each other might lead to a bursty L4S behavior and traffic pattern.

The congestion marking for uplink and downlink traffic is done in the PDCP layer of the gNodeB. For downlink traffic, the queueing delay is derived from internal feedback received from the RLC layer of the gNodeB. For uplink traffic, the combined PDCP and RLC queueing delay in the UE is estimated by the gNodeB based on Buffer Status Report (BSR), the current date rate and time between scheduling occasions. The gNodeB marks uplink packets on behalf of the UE and the UE is not required to support the L4S related congestion detection and CE marking functionality.

Every packet which is selected for marking is checked if the ECN bits in the IPv4 Type of Service (ToS) field or the IPv6 Traffic Class field are set to ECT-1(01). If the ECN bits are set to ECT-1, they are changed to CE (11) dependent on the threshold settings to indicate congestion. Otherwise, the existing ECN bit setting remains unchanged.

The client application echoes congestion information, for example the number of CE marked IP packets it has received, back to the server side. The server application in turn throttles the data rate proportional to the ratio of CE marked packets.

For TCP-based applications the echoing of congestion information is done by using the Accurate ECN (ACE) bits in the TCP header when sending TCP acknowledgements. The original ECN extension to IP as standardized by RFC 3168 is not suitable for the L4S operation on top of TCP. With the original ECN extension, receivers can only echo one congestion feedback signal per Round-Trip-Time (RTT) through TCP ACK messages irrespective of the number of CE marked packets arriving within the RTT. L4S senders need however more accurate feedback to adjust the data rate when operating on top of TCP transport protocols. Thus, TCP protocol stack implementations must provide more accurate ECN feedback according to the requirements outlined in RFC 9330 and RFC 7560. For applications using RTCP and RTP over UDP for the transport of real-time media, the congestion information is conveyed through RTCP feedback reports according to RFC 8888. Another transport protocol supporting L4S is QUIC (RFC 9000), where the ECN feedback including the number of CE marked packets is carried in ACK frames to senders.

L4S traffic must not be mixed with non-L4S traffic in the same QoS flow and is recommended to be transported in a dedicated QoS flow. E2e congestion control does not properly operate if L4S and non-L4S traffic share the same QoS flow and RLC queue, for example if the default QoS flow of a PDU session carries eMBB Internet traffic (non-L4S) and TCC traffic from L4S capable applications. This mutually influences the respective e2e congestion control operation and performance of the applications sharing the QoS flow.

Note: 5GC support is required to detect L4S traffic and trigger the setup of dedicated QoS flows using network-initiated session modification procedures.

For uplink traffic, the QoS flows carrying L4S and non-L4S traffic should be separated into dedicated Logical Channel Groups (LCGs). Since UEs report their uplink traffic demand with BSRs for each LCG, the non-L4S traffic portion might skew the RLC queue delay estimate for the L4S flow if both share the same LCG.

The L4S interworking with following NR RAN features and gNodeB functions must be considered:

- Data-aware Carrier Management For the buffer-based activation of SCells with NR downlink carrier aggregation, the L4sCfg.dlL4sLowThresh attribute value must be aligned with the CaSCellHandlingUeCfg.sCellActDeactDataThres to assure that the SCells are activated before L4S kicks in.
- NR-NR Dual Connectivity For downlink traffic, the L4sCfg.dlL4sLowThresh attribute value must be aligned with the NrdcMnCellProfileUeCfg.nrdcSetupDlPktAgeThr attribute value to assure that dual connectivity is established before L4S kicks in. Moreover, downlink PDCP Aggregation must be disabled by setting the DcDlCfg.dcDlAggAllowed attribute to FALSE. The L4S feature cannot properly determine a common RLC queueing delay for the two RLC entities on the Master and Secondary Nodes. For uplink traffic, the L4sCfg.ulL4sLowThresh attribute value must be aligned with the UlBufferMonCfg.ulBsVolThresh attribute value. Since the gNodeB determines the RLC queuing delay only for MCG traffic in FR1, there might be some impact on the uplink service performance if uplink PDCP aggregation is enabled. The impact depends on the distribution of uplink traffic across the MCG and SCG. If all traffic is sent across the SCG, the gNodeB will not perform any CE marking. This means that uplink L4S will not work if uplink PDCP aggregation is disabled, and the SCG is configured for the QoS flow.
- Active Queue Management (AQM) For downlink traffic, the L4sCfg.dlL4sHighThresh attribute value must be aligned with the AQM drop threshold attribute values, AqmCfg.tDiscardDl or AqmCfg.estimatedE2ERTT, to assure that L4S can adapt the data rate before AQM starts dropping packets. For uplink traffic, the UE uses the PDCP SDU discard timer to determine when to drop packets from the PDCP buffer. This timer is configured through the CUCP5qi.tPdcpDiscard attribute and must be aligned with the L4sCfg.ulL4sHighThresh attribute value. For additional information, see section Congestion Control - AQM below.

Congestion Control - AQM

For downlink traffic, congestion control is realized through Active Queue Management (AQM) techniques operating on the RLC and PDCP layers of the gNodeB.

Note: The choice whether AQM is performed on the PDCP or RLC layer is decided by the gNodeB implementation and depends on the UE connectivity scenario. For example, in dual connectivity scenarios using downlink PDCP aggregation, AQM is performed on the PDCP layer since only the PDCP layer has control over the transmissions across both cell groups used by Secondary Node terminated split DRBs.

Buffering and AQM play important roles in the performance of cellular networks to handle the data transmissions over the air interface under varying radio channel conditions. Long standing queues in large buffers degrade the performance of delay sensitive services while small buffers may result in a lower utilization of the air interface between the UE and gNodeB. An optimal balance between both must be found to meet the characteristics of the services being deployed.

The AQM operation for downlink traffic is controlled by a set of attributes which can be configured for each 5QI and DRB through the AqmCfg MO class. The main attribute to configure is the AqmCfg.aqmMode which can be set to following values:

- OFF No AQM is used. Packets are discarded if they become older than 1 second.
- MODE 1 Packets are discarded when their age is older than the minimum threshold calculated as 2 x AqmCfg.estimatedE2ERTT. A prohibit timer calculated as 4 x AqmCfg.estimatedE2ERTT is used between the discards of packets. All packets that are older than the maximum age threshold calculated as 10 x AqmCfg.estimatedE2ERTT are discarded.
- MODE 2 Packets older than the AqmCfg.tDiscardDl attribute value are discarded.

Note: For all modes, packets are discarded if the maximum number of packets that can be buffered for each DRB or by the gNodeB has been reached.

AQM MODE 2 is beneficial for service traffic which is tolerant to some packet loss but sensitive to latency violations and therefore all packets beyond a certain queueing delay threshold must be discarded. AQM MODE 1 is beneficial for services traffic where transmission reliability is more important than occasional latency violations and a less rigid packet drop behavior is needed.

To configure AQM on the RLC layer, the AqmCfg MO instance belonging to the GNBDUFunction must be referenced from the DU5qi MO instance that is representing a particular service traffic flow. For AQM on the PDCP layer, the AqmCfg MO instance belonging to the GNBCUUPFunction must be referenced from the corresponding CUUP5qi MO instance. To realize a homogenous AQM operation across the RLC and PDCP layers, both AqmCfg MO instances must be created with a uniform attribute setting.

Note: In NR-NR dual connectivity scenarios, the RLC and PDCP AQM configuration must also be consistent across the Master and Secondary Nodes.

If downlink L4S is enabled on the gNodeB, the AQM and L4S configuration must be aligned to prevent AQM from dropping packets before L4S kicks in. Following might be used as initial setting for the alignment:

- Downlink AQM MODE1 The AQM threshold at which the gNodeB starts dropping packets should be at least four times higher than the L4S high marking threshold: AqmCfg.estimatedE2ERTT &gt;= 2x L4sCfg.dlL4sHighThresh.
- Downlink AQM MODE2 The AQM threshold should be at least four times higher than the L4S high marking threshold: AqmCfg.tDiscardDl &gt;= 4 x L4sCfg.dlL4sHighThresh.

The margin between the L4S and AQM thresholds should be large enough to prevent AQM being triggered too early while L4S is still reacting on the congestion feedback when 100% of the IP packets are CE marked. The throughput and packet loss tolerance of service traffic flows might be considered to customize the margin. For example, a smaller margin might be used for low data rate traffic flows or traffic flows tolerating some packet loss and for which AQM MODE 2 is configured.

For uplink traffic, AQM operates on the PCDP layer in the UE and uses a PDCP SDU discard timer. At expiration of the timer, all PDCP SDUs and corresponding PDUs whose successful delivery has not been confirmed through PDCP status reports are removed from the buffer. The PDCP discard timer for uplink traffic can be configured through the CUCP5qi.tPdcpDiscard attribute. Uplink AQM is configured independently from downlink AQM and does not depend on the AqmCfg.aqmMode attribute setting.

If uplink L4S is enabled on the gNodeB, the PDCP discard timer and L4S high marking threshold must be aligned to prevent the UE PDCP layer from dropping packets before L4S kicks in. As initial setting for alignment following might be used:

- Uplink AQM The PDCP discard timer should be at least four times higher than the L4S high marking threshold: CUCP5qi.tPdcpDiscard &gt;= 4 x L4sCfg.ulL4sHighThresh.

#### 2.4.5.2 NR RAN Reliability Control

The RLC layer can enhance the reliability of data transmissions over the wireless interface by retransmitting missing or erroneous packets, meaning RLC SDUs. This is beneficial for services requiring data integrity and delivery assurance.

The RLC layer retransmits RLC SDUs based on the Automatic Repeat Request (ARQ) procedure for service traffic flows configured with Acknowledged Mode (AM). In Unacknowledged Mode (UM), missing or erroneous RLC SDUs are just ignored and considered as lost. As shown in the figure below, the RLC mode for uplink and downlink transmissions is configured through the CUCP5i.rlcMode attribute.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 39   Reliability Control in NR RAN

For traffic flows that can tolerate some packet loss, the attribute can be configured to Unacknowledged Mode (UM). This mode is suitable for many real-time media applications using RTP over UDP as application and transport layer protocols. Similar applies to IMS-based services like VoNR. In contrast, service flows requiring a highly reliable data transmission might perform better when using AM since it reduces the need for the e2e retransmissions if packets can be recovered through RLC retransmissions.

The RLC ARQ retransmission procedure complements the Hybrid ARQ (HARQ) retransmission procedure of the MAC layer. HARQ retransmissions are performed irrespective of the RLC mode. RLC retransmissions are triggered by the reception of an erroneous RLC PDUs which have been successfully transmitted over the MAC and physical layers. Retransmissions are also triggered by missing RLC PDUs after reaching the maximum number of HARQ retransmissions. The RLC retransmissions in AM are triggered when the t-Reassembly timer expires. This is the time the RLC layer waits for the successful delivery of RLC PDUs across the MAC and physical layers before requesting their retransmission by negatively acknowledging their reception in RLC status reports. With RLC UM, missing or erroneous PDUs are treated as lost when the timer expires.

Note: The reassembly timer does not delay the forwarding of RLC SDUs to higher layers. In both RLC modes, AM and UM, any non-segmented or successfully reassembled RLC SDU is immediately forwarded to the PDCP layer.

The timer values for uplink and downlink transmissions are configurable through the DrbRlcUeCfg.tReassemblyUl and DrbRlcUeCfg.tReassemblyDl attributes. When using UM, there is no performance benefit by customizing these timer values. When using AM, the values can be customized to speed up the RLC retransmission procedure.

However, the timer values must always be aligned with the number of HARQ (re)transmissions that the MAC layer is allowed to perform. For example, if the MAC layer is configured to perform up to 5 transmission attempts for every transport block, the RLC reassembly timer should not be tuned to only allow for a lower number of HARQ transmissions. There might also be the risk of duplicate retransmissions when using RLC AM and the RLC reassembly timer is set too low. In this case a RLC retransmission might be triggered while HARQ retransmissions are still ongoing.

The number of HARQ transmissions can be customized through the HarqUeCfg.ulHarqMode and HarqUeCfg.dlHarqMode attributes. The number of required HARQ transmissions in turn depends on the radio conditions and Link Adaptation configuration that is used for a service traffic flow. For additional details, see  Link Adaptation.

Note: RLC operations like segmentation and reassembly or status reporting in AM also depend on the data rates of services and benefit from a differentiated configuration. See QoS Framework, for the configuration of RLC attributes differentiating between low and high data rate services.

#### 2.4.5.3 NR RAN Reordering Control

The PDCP entities in the UE and gNodeB are responsible for the sequencing and reordering of packets. The PDCP layer can receive RLC SDUs in an unordered sequence due to packet transmissions along different paths or packet retransmissions performed by lower protocol layers. Based on the PDCP sequence number, the PDCP layer reorders RLC SDUs if received within a certain time window.

The reordering operation for uplink and downlink traffic is controlled by the attributes shown in the figure below.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 40   Reordering Control in NR RAN

For downlink traffic, the time window considered by the PDCP layer on the UE is configurable through the CUCP5qi.tReorderingDl attribute. Any out-of-order RLC SDU which arrives after the expiration of this timer is discarded by the UE PDCP layer.

For uplink traffic, the time window considered by the PDCP layer on the gNodeB is configurable through the CUCP5qi.tReorderingUl attribute. When the reordering timer expires, all RLC SDUs that already have been received are forwarded to higher layers. The time window can be extended to wait for the delayed arrival of out-of-order RLC SDUs. This extension time is controlled with the AqmCfg.tOooUlDelivery attribute. The PDCP layer discards out-of-order RLC SDUs which are not received within the time window determined by the sum of the CUCP5qi.tReorderingUl and AqmCfg.tOooUlDelivery attribute values.

Reducing the PDCP reordering timer helps to improve the latency performance by avoiding packets being buffered for a too long time.

The AqmCfg.tOooUlDelivery timer does not influence the latency performance in the RAN. Therefore, the pre-configured default value must not be changed.

Note: Configuring the AqmCfg.tOooUlDelivery attribute to low values might impact the performance of traffic flows requiring reliable data transmissions. This is the case if the PDCP layer drops delayed out-of-order RLC SDUs too early and thereby triggers e2e retransmission procedures reacting on packet loss.

The PDCP reordering timers must be configured according to the latency requirements of the traffic flows and consider the operation of lower protocol stack layers. For example, the PDCP reordering timer for downlink transmissions (CUCP5qi.tReorderingDl) must not be lower than the RLC reassembly timer (DrbRlcUeCfg.tReassemblyDl) to prevent that the PDCP layer drops RLC SDUs arriving after the reordering timer expired. When RLC AM is used, the downlink PDCP reordering timer can be configured to larger values, for example 100 ms, to accommodate RLC retransmissions or interruptions caused by UE handover procedures. In short:

- Downlink PDCP Reordering Timer with RLC UM CUCP5qi.tReorderingDl &gt;= DrbRlcUeCfg.tReassemblyDl and CUCP5qi.tReorderingDl &lt;= RAN PDB required for the service
- Downlink PDCP Reordering Timer with RLC AM CUCP5qi.tReorderingDl &gt; DrbRlcUeCfg.tReassemblyDl plus the time required for RLC retransmissions and CUCP5qi.tReorderingDl &lt;= RAN PDB required for the service

For uplink transmissions, the PDCP extension period (AqmCfg.tOooUlDelivery) provides extra time to handle the forwarding of RLC SDUs arriving after the PDCP reordering timer expired. Therefore, the PDCP reordering timer (CUCP5qi.tReorderingUl) can be set to low values, for example 20 ms, to forward received RLC SDUs as early as possible. There is no need to align the PDCP reordering timer with the RLC reassembly timer. However, services might be sensitive to the reception of out of order packets. Hence, the CUCP5qi.tReorderingUl timer should not be set to extremely low values to assure that most packets are delivered in-order to the application layer. In short:

- Uplink PDCP Reordering Timer with RLC UM and AM CUCP5qi.tReorderingUl &gt;= Minimum time required for the in-order delivery of packets and CUCP5qi.tReorderingUl &lt;= RAN PDB required for the service

#### 2.4.5.4 NR RAN Service-Specific Configuration

In general, it is recommended to align the attribute settings between the radio protocol stack layers to support an efficient RAN operation and optimal service performance.

To optimize the performance of latency sensitive services, it is important to configure the RAN in a way to provide low queuing delays while maintaining a high air interface utilization. The configuration must be done according to the performance characteristics of the respective service flows considering, for example:

- The data rate of the service flow.
- The ability to perform rate adaption based on E2e congestion control mechanisms.
- The tolerance to packet loss and retransmission capabilities.
- The latency boundaries for uplink and downlink transmissions.

If the QoS flow of a service consists of multiple traffic types with different performance characteristics, the RAN operation might be tuned towards the traffic type with the most stringent performance requirements or towards an average satisfying most of them. Ideally, traffic types with different performance characteristics should be separated into dedicated QoS Flows and DRBs.

Note: The performance requirements of QoS flows might not be symmetric and must be considered separately for uplink and downlink.

To provide some high-level guideline for the configuration of the congestion control, reliability, and reordering operation in NR RAN, following service categories are introduced:

- Rate-adaptive services for which reliability is more important than latency Includes services that can adapt the data rates of their traffic flows based on flow control or congestion feedback information. Although their latency performance must be controlled and stay within certain limits, they additionally require reliable transmission with low packet loss rates. Certain media production services providing high-quality media formats might belong to this category. For example, the production and reliable delivery of (live) media into a Content Delivery Network (CDN) for the further distribution to large audiences. The first mile delivery of the media into the CDN must be done with reliability to provide the content with high quality to consumers who are streaming the media from the CDN. The assumption is that the services use transport layer protocols, like TCP, or application layer protocols providing data retransmission capabilities.
- Rate-adaptive services for which latency is more important than reliability Includes services that can adapt the data rates of their traffic flows, require latency control to maintain their real-time characteristics, but can tolerate some packet loss. Certain cloud gaming services might belong to this category. The assumption is that the services use transport layer protocols without retransmission capabilities, like UDP.

In contrast to rate-adaptive service flows, service flows whose data rates cannot be adapted are particularly vulnerable to the impact of congestion and may experience degraded performance without effective flow and congestion control provided by the network. Non-adaptive service flows are mainly connected to the industrial control and remote-control use case families which are not considered in this document. Most of the real-time media services have rate adaptation capabilities for the uplink or downlink traffic flows carrying video and audio data. Certain real-time media services, like cloud AR or interactive video conferencing, might perform rate adaption in both directions with or without L4S support.

Note: Real-time media services might also contain non-adaptive traffic flows, for example the uplink user input to control cloud gaming services. These flows typically have a low data rate. With respect to congestion control and PDCP reordering, they might be configured according to their intrinsic latency and reliability requirements.

For rate-adaptive flows requiring reliable data transmissions, RLC AM is preferred over UM to locally handle the retransmission of packets that are lost in RAN instead of triggering e2e retransmissions. If they support L4S as congestion control algorithm, the related L4S thresholds in RAN should be configured according to their throughput and latency requirements. Without L4S support, they might benefit from a selective AQM operation that occasionally drops some buffered packets to early trigger e2e congestion control performing loss-based rate adaptation and retransmission. This avoids mass packet loss and retransmissions if the maximum buffer threshold is reached. AQM MODE 1 is recommended for the rate-adaptive flows with the drop thresholds being configured according to their e2e latency requirements. The PDCP reordering timers must also be set according to the latency requirements of respective service traffic flow and consider the impact of RLC retransmissions. For uplink transmissions, the reordering extension period configured through the AqmCfg.tOooUlDelivery attribute might be customized dependent on the anticipated E2e congestion control algorithms, like Cubic function Binary Increase Congestion (CUBIC), Bottleneck Bandwidth and Round-Trip propagation time (BBR), or others. With e2e congestion control algorithms providing fast and efficient recovery strategies to control the amount of data in flight when detecting packet loss, lower values might improve the service performance. If the algorithm is unknown, the default value should be kept for this attribute.

For rate-adaptive flows that tolerate some packet loss, RLC UM and AQM MODE 2 is preferred to improve their latency performance. If they support L4S as congestion control algorithm, the related L4S thresholds in RAN should be configured according to their throughput and latency requirements. Without L4S support, the AQM drop threshold should be configured according to the e2e latency requirements of the service traffic flows. The PDCP reordering timers must also be set according to the latency requirements for uplink and downlink transmissions. There is no need to tune the reordering extension period using the AqmCfg.tOooUlDelivery attribute for this category of services.

The table below shows a high-level configuration example for each service category:

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 41   High-Level NR RAN Configuration Example for each Service Category

(1) The assumption is that RLC retransmissions do not violate the latency requirements of the service flows and that RLC retransmissions are preferred over e2e retransmissions being triggered if packet loss is detected. The RLC retransmission procedure can be optimized by tuning the relevant timers like t-Reassembly or t-PollRetransmit which are configurable through corresponding DrbRlcUeCfg MO class attributes.

(2) Setting the low threshold to 1/3 of the high threshold just reflects an initial starting point which can be customized towards larger or smaller differences based on the observed service performance.

(3) Since the minimum threshold where packets are going to be discarded is calculated as 2 x estimatedE2ERTT, the value for the attribute should be configured lower than the RTT, for example, half of the e2e RTT. Note, that lower estimatedE2ERTT attribute values decrease the throughput and may increase the latency by triggering retransmissions on high layer protocols.

The NR Service-Adaptive User Plane Profile feature allows configuring the RAN operation for congestion control, reordering and transmission reliability dependent on the performance characteristics of individual service flows. The corresponding attributes can be customized for each 5QI and DRB. If all service flows in the mobile network are separated by using different 5QIs, there is no need create additional UE groups beyond the Base UE Group.

The figure below shows a configuration example that is differentiating the RAN operation between a TCC service, like cloud gaming, and an eMBB service. The downlink traffic flow of the cloud gaming service is rate-adaptive supporting downlink L4S and latency is more important than reliability. For the non-adaptive uplink flows including, for example, the user input for game control, the same one-way latency as for downlink is assumed.

Note: The figure also shows MO instances that are automatically created and referenced if the corresponding reference attribute is not configured. Features which are per default disabled, like L4S, also do not require the creation of an explicit MO instance and configuration of corresponding reference attribute for service flow DRBs not requiring L4S support, like 5QI 9.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 42   Flow and Congestion Control Configuration Example

For the cloud gaming service, a simple approach is used to determine an example configuration for the AQM, downlink L4S and PDCP reordering attributes.

The upper e2e one-way latency boundary for the uplink and downlink transmissions of cloud gaming services is assumed to be 30 ms. Using a Mobile Edge Cloud (MEC) deployment, where the application servers are located close to the RAN, the transport network latency can be ignored. The 5GC is assumed to have sufficient processing and networking capacities and therefore most of the latency budget is contributed by the RAN. For example, 4/5 of the e2e latency budget, meaning ~24 ms, is used as packet delay budget (PDB) for the uplink and downlink transmissions in RAN.

Note: The Quality of Experience (QoE) of cloud gaming services might not be severely impacted by occasional latency spikes beyond the PDB. Therefore, it is not required that 100% of the packets stay within the PDB.

Based on the information above, the initial AQM, downlink L4S and PDCP reordering attribute values are derived. For attributes that can only be configured with discrete values, for example CUCP5qi.tReorderingUl or CUCP5qi.tPdcpDiscard, the RAN PDB is rounded up or down to the nearest value that can be configured.

Note: The attribute values do not represent a recommended configuration but just an example configuration for a hypothetical cloud gaming service. In practice, these attributes must be set and customized according to the observed performance dependent on the given service and RAN deployment.

- L4S
    - Downlink: L4sCfg.dlL4sHighThresh = 24 ms (&lt;= RAN PDB), L4sCfg.dlL4sLowThresh = 8 ms (L4sCfg.dlL4sHighThresh / 3)
    - Uplink: N/A
- AQM
    - AqmCfg.aqmMode = MODE2
    - Downlink: AqmCfg.tDiscardDl = 100 ms (&gt;= 4 x L4sCfg.dlL4sHighThresh)
    - Uplink: CUCP5qi.tPdcpDiscard = 50 ms (&gt;= 2x RAN PDB)
- PDCP
    - Downlink: CUCP5qi.tReorderingDl = 20 ms (&lt;= RAN PDB)
    - Uplink: CUCP5qi.tReorderingUl = 20 ms (&lt;= RAN PDB), AqmCfg.tOooUlDelivery = 150 ms (default)
- RLC
    - CUCP5qi.rlcMode=UM
    - Downlink: DrbRlcUeCfg.tReassemblyDl = 20 ms (&lt;= CUCP5qi.tReorderingDl)
    - Uplink: DrbRlcUeCfg.tReassemblyUl = 20 ms (= DrbRlcUeCfg.tReassemblyDl)

The AQM threshold for uplink traffic (CUCP5qi.tPdcpDiscard) is set to a higher value than the RAN PDB allowing a higher uplink queuing delay in favor of dropping packets too early. For cloud gaming applications that need reliable user feedback, the corresponding AQM attribute (CUCP5qi.tPdcpDiscard), can even be set to higher values, for example 100 ms.

For the eMBB service, the pre-configured default values for 5QI 9 in NR RAN are used.

### 2.4.6 Observability

The TCC service performance in mobile networks can be observed by using active or passive measurement methods, as shown in the figure below.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 43   Observability - Measurement Methods

Active measurement methods include measurement applications installed on UEs or traffic analysis tools connected to the mobile network which can send probing traffic across the network to evaluate its performance. UE measurement applications send the probing traffic E2e across all mobile network domains towards servers in the connected the Data Network. In contrast, traffic analysis tools are connected to interfaces inside the mobile network, for example N3 and N6, and evaluate the performance of individual network segments within or across domains. The probing traffic often consists of some artificial traffic profile, for example, ICMP-ping packets of a certain size or data chunks of varying sizes to test the uplink and downlink performance in terms of throughput and Round-Trip Time (RTT).

Publicly available performance data gathered through UE measurement applications, like Ookla SpeedTest, is is referred to as crowdsourced data. The performance data can be inspected per country, location, network operator, and radio access technology. In NR SA deployments such crowdsourced data does not reliably represent the performance of a mobile network if the measurements are taken for a different network slice and QoS flow than the one used by individual service applications. This is the case for services like TCC since their performance relies on a customized RAN configuration of the corresponding QoS flows in the CIoT network slice. In consequence, the PDU session established by measurement applications must be steered to the correct network slice and the QoS flow of the PDU session must use the 5QI value of the service whose performance is of interest.

In practice, the UEs running measurement applications must have a subscription to the same network slice and DNN used by TCC services. If the UE has subscriptions to multiple network slices, URSP rules are needed to steer the PDU session of the measurement application. However, with public measurement applications it might not be possible to flexibly select the network slice since the operator controlled URSP rules will always steer the PDU session of the measurement application to a particular network slice. If TCC services establish dedicated QoS flows which are handled with an optimized configuration, the measurement application might not measure the performance with the correct QoS setup in RAN when using the 5QI value of the default QoS flow. Moreover, for reliable performance data, the traffic profiles used by measurement applications should ideally resemble the ones of the real services.

Passive measurement methods include Performance Management or Monitoring (PM) information provided by the network elements in the different domains of the mobile network. This PM information is collected by the network elements based on the current user or service traffic flowing through the network. Traffic analysis tools connected to different network interfaces might be used to trace the ongoing traffic flows and provide additional PM information. The collected PM information can be fed into Performance Analysis tools to determine and visualize the required Key Performance Indicators (KPIs). Like with active measurement methods, it is also crucial to filter or correlate the collected PM information with the specific network slice and QoS flow to evaluate the performance of the corresponding services.

This section focuses on the passive measurement method to observe the latency performance for TCC services in NR RAN. In particular, the performance information provided the gNodeBs or Ericsson Network Manger (ENM) in form of PM or Event Based Statistics for NR (EBS-N) counters is described. The EBS-N counters, which are generated by the ENM based on PM event information received from the gNodeB, can be filtered per network slice, UE group and QoS (5QI). In contrast, the PM counters created by the gNodeB can at most differentiate the performance data for the group of DRBs or QoS flows irrespective of the network slice deployed in the NR Cell.

The PM information collected from EBS-N or PM counters can be used to determine certain Performance Indicators (PI). PIs represent metrics showing how specific parts of the RAN perform, for example, how much downlink queueing delay is added by the RLC layer in the gNodeB. Depending on the counter, the PI is provided per NR cell, DRB or UE for each ROP and might be aggregated over multiple ROPs or cluster of cells. PIs can be used to determine Key Performance Indicators (KPIs). KPIs represent the performance and efficiency of the RAN on network level and impact the end-user perception how the network performs in general. The relevance and impact of PIs measuring the RAN delay on latency KPIs requires further investigation with different delay sensitive applications. Latency KPI information is provided when this impact is better understood.

Moreover, there is an inherent relation between the throughput and latency performance of services. If the throughput performance of services is poor due to cells being highly loaded or providing poor coverage, the latency performance is equally poor for the same reason. In contrast, if the throughput performance is good but the required bounded latency is not met, further analysis and optimization is required. It is recommended to analyse latency performance mainly for QoS flows and NR cells where the services can achieve a certain throughput performance, for example the minimum throughput required for the service to operate. Besides the cell capacity, duplex scheme (FDD or TDD), traffic characteristics and radio coverage other factors like the RAN feature and QoS configuration influence the latency performance. The feature configuration and QoS setup in RAN can be tuned to meet the required service performance.

Related Information

For general information about the performance data types, PM and EBS-N counters for NR, and the network slice-specific filtering of EBS-N counters, see:

- Manage Performance NR
- Lists and Delta Lists - Referral Document
- NR SA Network Slicing Guideline

#### 2.4.6.1 NR RAN Latency Performance

The 3GPP Technical Specification for 5G Performance Measurements (TS 28.552) defines the terms IP latency and packet delay. IP latency is the time it takes to transfer an initial packet in a data burst from one point to another while packet delay refers to the time it takes to transfer any packet between two points. In the remainder of this section the term packet delay or delay is used, assuming that the service performance and user's Quality of Experience is impacted by the number of packets violating the bounded latency requirement irrespective of their position in a data burst. This section provides an overview of the components that contribute most to the uplink and downlink delay in NR RAN together with their relation to the corresponding radio protocol stack layers and available PM and EBS-N counters.

NR RAN Downlink Delay

The figure below shows the downlink delay components and the available gNodeB PM and EBS-N counters measuring their delay contribution. Additionally, the counters measuring the transmission reliability on the MAC and RLC layers are provided since HARQ and ARQ retransmissions impact the delay performance of services.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 44   Downlink Delay Components in NR RAN

According to 3GPP TS 38.314, the downlink delay components consist of following:

- PDCP Delay (D4) Time between the arrival of a PDCP SDU at the PDCP ingress NG-U termination point and the corresponding PDCP PDU being send to the DU function over the egress F1-U/Xn-U termination point. The PDCP delay is mainly relevant in dual connectivity scenarios using downlink PDCP aggregation when flow control is active and controlling the data rates across the MCG and the SCG for Secondary Node split DRBs.
- F1-U Delay (D3) Half of the time it takes to send a GTP-U packet from the CU-UP egress GTP termination point to the DU ingress GTP termination point plus the corresponding data delivery status feedback received by the CU-UP function. The F1-U delay is mainly relevant in split gNodeB deployment scenarios where the CU-UP and DU functions are inter-connected by an external transport network.
- RLC Delay (D2) Time between the arrival of a RLC SDU at the RLC ingress F1-U termination and the last part of the SDU being sent to the MAC layer for transmission. This component represents the queuing delay in the RLC layer and is the largest contributor to the downlink delay. If the radio interface is the bottleneck, the packets of each UE DRB are queued up in the corresponding RLC buffer. Note: The additional delay due to RLC retransmissions in AM is not included.
- Air interface delay (D1) The time needed to schedule all segments of a RLC SDU to the UE until receiving the HARQ ACK that confirms the successful transmission of the last segment. For reliable transmissions using RLC AM, the reception of the RLC ACK is considered instead of the HARQ ACK.

The downlink delay introduced by the D4 and D3 components can currently not be measured through PM or EBS-N counters. These components can be ignored for integrated (non-split) gNodeB deployments and services not using NR-NR Dual Connectivity with downlink PDCP aggregation.

The PM and EBS-N counters related to the delay component D1, pmMacLatTimeDlxyz and pmEbsnMacLatTimeDlxyz, measure the time between the reception of packets in an empty buffer and scheduling the first packet in a burst of packets. In contrast to the 3GPP definition of D1, the counters do not measure the air interface delay for each packet and do not consider any HARQ retransmission overhead. These counters might serve as Performance Indicators (PI) to measure the delay of low data rate services when not much queueing is taking place, and all packets are sent to the UE with one or a few transmissions. For these services, the experienced delay in RAN is mainly reflected by the initial scheduling delay of the first packet.

However, if there is resource contention and packets start to queue up in the RLC layer, the RLC queuing delay represented by the D2 component is the dominant factor impacting the service delay. This is especially the case for high data rate services which continuously send packet bursts of varying sizes. The initial scheduling delay of the first packet is then negligible compared to the RLC queuing delay. The PM and EBS-N counters pmRlcDelayTimeDlxyz and pmEbsnRlcDelayTimeDlxyz measure the RLC queueing delay for each packet. The EBS-N counters filtered by the CIoT network slice identifier are recommended to observe the downlink delay performance of real-time media services in NR RAN. The EBS-N counters can additionally be filtered for each QoS flow if the observability must be differentiated across different services in the same network slice.

As shown in the figure below, the EBS-N counters pmEbsnRlcDelayTimeDlDistr and pmEbsnRlcDelayTimeDlMaxDistr record the distribution of the average and maximum downlink RLC latency for each DRB and NR Cell by using the PM event: DuPerUeRbTrafficRepAggr10. The PM event aggregates the RLC delay data over periods of 10 seconds.

Note: For the maximum downlink RLC latency distribution, the PM event field per\_rb\_rlc\_delay\_time\_dl\_max\_list consists of an array of 10 values representing the maximum delay for each second in the 10 s aggregation period. These values are individually mapped to the corresponding EBS-N counter bins to reflect the maximum delay distribution for each second in the Result Output Period (ROP).

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 45   EBS-N Counters measuring the Downlink RLC Delay Distribution

The pmEbsnRlcDelayTimeDlMaxDistr counter provides a better resolution to detect RAN PDB violations compared to the pmEbsnRlcDelayTimeDlDistr counter which averages the delay over a 10 s period. The pmEbsnRlcDelayTimeDlDistr counter does not allow detecting individual packets violating the RAN PDB within this 10 s period. The pmEbsnRlcDelayTimeDlMaxDistr counter does not reveal how often the RAN PDB is violated for each second, since only the packet sample with the maximum delay is considered.

To determine the overall average downlink RLC delay for each QoS flow and NR Cell in the measurement ROP, following EBS-N counters and formula can be used:

Note: Averaging the delay information masks RAN PDB violations of individual packet samples during the ROP. Therefore, the distribution counters are generally preferred to observe the latency performance of services.

To include an estimate for the additional delay caused by HARQ retransmissions, the average downlink RLC delay can be multiplied with the Block Error Rate (BLER):

The BLER is calculated from the corresponding HARQ ACK and NACK ratio in the same ROP with following counters:

Similarly, an estimate for the additional delay caused by RLC retransmissions for flows using RLC AM can be added multiplying the average downlink RLC delay with (1 + ARQ-Ratio) where the ARQ-Ratio is determined from following counters:

Adding the HARQ and ARQ retransmission ratios to the average downlink RLC delay might overestimate the downlink delay since the RLC queueing delay is implicitly impacted by the prioritized handling of retransmissions in the MAC and RLC layers.

Note: HARQ retransmissions are scheduled in the MAC layer by using a higher priority than the one used for any active DRB. The RLC layer drains retransmission data with higher priority from the buffer to the MAC layer than normal data.

The HARQ and ARQ retransmission overhead can also be added to the weighted average downlink RLC delay calculated from the samples and center values of the bins used by the pmEbsnRlcDelayTimeDlDistr EBS-N counter. The retransmission overhead cannot be added to the weighted average maximum delay calculated from the pmEbsnRlcDelayTimeDlMaxDistr bins since it would accumulate the retransmission overhead of all samples to a few samples representing the maximum delay. Moreover, the PM and EBS-N counters report the HARQ and ARQ acknowledgements on UE level and regardless which QoS flows use RLC AM or UM.

Note: The RLC ARQ counters are only stepped if the UE runs at least one QoS flow in RLC AM.

For the HARQ retransmission estimate, there might be no need to differentiate between UEs running TCC and non-TCC QoS flows if their distribution in the NR Cell and BLER targets are similar. Otherwise, the HARQ ACK and NACK EBS-N counters must be filtered, for example by using a UE group which includes the 5QI value as part of the group selection criteria. The filtering helps to remove the retransmission bias from UEs not running TCC services.

The figure below shows this filtering approach.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 46   EBS-N Counter Filtering removing the BLER Bias from Non-TCC UEs

The downlink RLC delay counters are filtered by the PlmnId, Snssai and QoS (5QI) value while the HARQ ACK and NACK counters are filtered by the PlmnId and respective UE Service Group Id matching UEs running a specific QoS flow.

For example, UEs running QoS flows with 5QI 7 are selected into CIoT UE Group 1, removing the BLER bias from UEs running other QoS flows. If a single UE runs multiple QoS flows and different UE groups with selection criteria containing the corresponding 5QI values exist, the UE is selected into the UE group with the highest UE group priority. For example, a UE running QoS flows with 5QI 7 and 5QI 80 is selected into the CIoT UE Group 2 adding the BLER of this UE to the average downlink RLC delay measured for the QoS flow with 5QI 80. For the RLC retransmission estimate, the QoS flows using RLC AM and UM cannot be separated by means of filtering. In consequence, the RLC retransmission estimate of QoS flows using AM would be added to the downlink RLC delay of QoS flows using UM. The RLC retransmission estimate can only be added to the average downlink RLC delay of QoS flows using AM.

In summary:

- The RLC Delay (D2) is the main contributor to downlink delay. The delay can be observed with the EBS-N counters: pmEbsnRlcDelayTimeDlDistr and mEbsnRlcDelayTimeDlMaxDistr. These counters mirror the E2e downlink delay trend on service level if the RAN is the bottleneck. The counters must be filtered for the network slice hosting the TCC services. If needed, they can additionally be filtered by the used QoS (5QI) in the network slice.
- The effect of HARQ retransmissions might be added to the average downlink RLC delay by multiplying the average delay value with the BLER. EBS-N counter filtering might be needed to remove the BLER bias from UE groups that do not run TCC services.
- The effect of ARQ retransmissions might be added to the average downlink RLC delay. This is only possible for groups of UEs running QoS flows in RLC AM.
- Averaging the RLC delay information masks occasional RAN PDB violations.

NR RAN Uplink Delay

The figure below shows the uplink delay components and the gNodeB PM and ENM EBS-N counters that are currently available to measure their delay contribution. Additionally, the counters measuring the transmission reliability on the MAC and RLC layers are provided.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 47   Uplink Delay Components in NR RAN

According to 3GPP TS 38.314, the uplink delay components include following:

- PDCP Queueing Delay (D1) Time between the arrival of a PDCP SDU at the UE PDCP layer until the uplink MAC PDU containing the first part of the PDCP SDU is scheduled for transmission. This includes the delay for the UE to get the first resource grant after sending a SR. For a larger burst of packets (PDCP SDUs), the delay between sending out BSRs and receiving grants for the transmission of the remaining PDCP SDUs in the UE buffer needs to be measured. To differentiate both cases the D1 component is divided into a D1-Initial (D1-I) sub-component and a D1-Consecutive (D1-C) sub-component. D1-I covers the time for a PDCP SDU arriving in an empty UE buffer until receiving the SR grant for transmission, that means the data indication or SR-to-Grant delay. D1-C covers the time to schedule consecutive grants after receiving the BSR for the transmission of the following PDCP SDUs. Hence, D1-C captures the scheduling delay if the size of the SR grant is too small to transmit the complete burst of packets. This might be the case if the configured BsrUeCfg.srGrantSize is too small or if there is resource contention on the air interface. The D1 component is the largest contributor to the uplink delay. If the radio interface is the bottleneck, packets are queued up in the UE PDCP buffer. Note: The UE must have the capabilities to measure the D1 component by means of Minimization of Drive Test (MDT) procedures supporting the M6 uplink delay measurements outlined in 3GPP TS 37.320. The gNodeB can measure the D1-I and D1-C sub-components on behalf of the UE to cover the scheduling delay for SR and BSR requested uplink grants.
- Air interface delay (D2.1) Time between scheduling an uplink MAC SDU in the UE MAC layer with the provided scheduling grant until the MAC SDU is successfully received by the RLC layer in the gNodeB. This includes the delay due to HARQ retransmissions.
- RLC Delay (D2.2) Time between the arrival of the RLC PDU including the first part of a RLC SDU at the RLC layer and the complete RLC SDU being sent to the PDCP layer in the gNodeB. This includes the delay due to ARQ retransmissions of RLC SDUs in AM.
- F1-U Delay (2.3) Half of the time it takes to send a GTP-U packet from the DU egress GTP termination point to the CU-UP ingress GTP termination point plus the corresponding data delivery status feedback received by the DU function. The F1-U delay is mainly relevant in split gNodeB deployment scenarios where the CU-UP and DU functions are inter-connected by an external transport network.
- PDCP Reordering Delay (D2.4) Time between the arrival of an uplink PDCP PDU at the PDCP layer until the corresponding PDCP SDU is forwarded to higher layers. The main delay for this component is introduced due to the reordering of PDCP SDUs arriving out of order. This can happen in NR-NR Dual Connectivity scenarios using uplink PDCP aggregation or due to retransmissions performed by lower radio protocol stack layers, for example MAC HARQs.

The uplink delay introduced by the D2.4, D2.3 and D2.2 components can currently not be measured with PM or EBS-N counters. For D2.2, the number of ARQ retransmissions on the RLC layer can be observed for UEs running QoS flows in RLC AM. Again, the ARQ-Ratio cannot be factored into the measured uplink RLC delay of QoS flows since it is not possible differentiate QoS flows using RLC AM and UM. While the delay introduced by D2.3 can be neglected for integrated (non-split) gNodeB deployments, the PDCP reordering delay introduced by D2.4 depends on the connectivity scenario and conditions triggering retransmissions in lower protocol layers. Avoiding connectivity scenarios using NR-NR Dual Connectivity with uplink PDCP aggregation reduces the likelihood of out of order packet deliveries. The PDCP reordering delay is limited by CUUP5qi.tReorderingUl time. This is the maximum time that received PDCP PDUs are buffered to wait for the arrival of an out of order PDU before being forwarded. For details, see  NR RAN Reordering Control.

The most significant delay is introduced by the D1 component. PM and EBS-N counters measuring the sub-components D1-I and D1-C are available. However, it must be understood what type of delay they measure to decide upon their applicability to observe the delay performance of the corresponding services. In general, the counters measure the delay from the perspective of the gNodeB. The figure below shows some details about the PM and EBS-N counters for D1-I and D1-C based on a UE sending 3 PDCP PDUs on a FDD carrier.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 48   Uplink Delay Components - D1-I and D1-C Counters

Following PM and EBS-N counters measure the D1-I sub-component, the so-called SR-2-Grant delay which is the time between the gNodeB receiving the SR from the UE and scheduling the corresponding PDCCH grant to the UE:

- pmMacPucchSrPuschGrantLatDistr
- pmEbsnMacPucchSrPuschGrantLatMaxDistr

For services with low uplink bitrates where the air interface is not restricting the uplink transmissions and the size of the SR grant is big enough to empty the buffer, the SR-2-Grant delay is the main delay contributor. The SR-2-Grant delay might be ignored for uplink heavy services where the UE scheduling is restricted for a longer time.

Note: The grant size provided by the gNodeB can be customized with the NR Adaptive Uplink Grant Size feature.

Following information must be considered when using the SR-2-Grant delay counters:

- The effect of using different SR periodicities for different services based on the NR Service-Adaptive SR Periodicity feature configuration is not included.
- The SR-2-Grant delay is measured on UE level. When using EBS-N counters, filtering must be applied to only consider the SRs from UEs with active DRBs used for TCC services. For example, the EBS-N counter pmEbsnMacPucchSrPuschGrantLatMaxDistr can be filtered by the UeServiceGrpId where the corresponding UE Service Group contains the 5QI value of the TCC service as selection criteria. However, for UEs that have established multiple DRBs, it is unknown for which DRB and related service the SR was sent.
- The SR-2-Grant delay during PDCCH resource contention depends on the scheduling priority of the QoS flow which can be controlled if the QoS-Controlled SR Scheduling function is enabled and the SrHandlingUeCfg.srHandlingMode attribute is set to SR\_PRIO\_5QI. For additional details, see QoS-Controlled SR Scheduling.

The PM and EBS-N counters related to the D1-C sub-component are divided into different parts:

- PM and EBS-N counters measuring the initial scheduling delay or 1st BSR-2-Grant delay:
    - pmMacInitSchedDelayUlMaxQos
    - pmEbsnMacInitSchedDelayUlMaxDistr
- PM and EBS-N counters measuring the contention delay and restricted scheduling time. These counters consider the time to schedule a series of BSR requested grants on the PDCCH until the UE buffer is empty:
    - pmMacContentionDelayUlDistr
    - pmEbsnMacTimeUlResDrb (excluding the last PDCCH grant emptying the buffer)
- PM and EBS-N counters measuring the duration while the DRB of a UE is in restricted scheduling state. Restricted scheduling ends with the PDCCH grant emptying the UE buffer or when the UE reports a BSR with a size of zero: Note: The delay for scheduling BSR requested grants depends on the QoS configuration of the QoS flow. The scheduling priority for the grants is determined by the QoS flow related DRB which has the highest priority in the corresponding Logical Channel Group (LCG) contained in the BSR.
    - pmMacTimeUlResDrbMaxQos
    - pmEbsnMacTimeUlResDrbMax
    - pmEbsnMacTimeUlResDrbMaxDistr

The D1-I and D1-C counters above are specialized counters which might be useful to analyze the uplink scheduling behavior of UEs in more detail. Otherwise, the following PM and EBS-N counters are sufficient to monitor the trend in the experienced E2e uplink delay of services:

- pmEbsnRlcDelayTimeUlMax
- pmEbsnRlcDelayTimeUlMaxDistr

These EBS-N counters report the maximum estimated delay of the oldest packet in the uplink queue of a UE. In practise, they measure the delay introduced by the D1-C sub-component and partially also the D2.1 component. The gNodeB estimates the uplink RLC delay based on the current data rate, the UE buffer size reported through BSRs, and the time between PDCCH grant scheduling occasions. The latter is referred to as inter-scheduling delay. However, the SR-2-Grant delay is not considered. If SR grants are big enough to empty the UE buffer, the counters report zero delay since packets do not queue up in the UE buffer. If SR grants do not empty the UE buffer, the counters measure the packet delay starting from the scheduling of the 1st BSR grant and including the inter-scheduling delay for all succeeding grants until the packet (PDCP PDU) is completely received. For the first packet, the inter-scheduling delay to the SR grant is also included. For the next packet, the measurement continues with the PDCCH grant that delivers the first part of the packet until the packet is completely received. The packet with the longest delay among all transmitted packets reflects the uplink RLC delay of the oldest packet for the given measurement period.

Following information must be considered when using the uplink RLC delay counters:

- The uplink delay is estimated for each LCG. In consequence, the DRBs for TCC services should be separated from DRBs for non-TCC services by placing them into an own LCG. This avoids skewing the delay estimate due to the mix of traffic from different service DRBs in the same LCG. The DRBs for each QoS flow can be assigned to LCGs by configuring the DU5qi.logicalChannelGroupId or the LogicalChannelUeCfg.logicalChannelGroup attribute. The configuration of the latter attribute requires the activation of the Enhanced UE QoS Control in Uplink feature.
- The effect of MAC HARQ or RLC ARQ retransmissions is indirectly included since the prioritized handling of retransmissions results in a lower data rate of new packets received by the gNodeB. If the uplink RLC counters report unexpectedly high delay values, the corresponding PM or EBS-N counters related to the HARQ and ARQ retransmissions should be checked. Note: The HARQ and ARQ counters measure the retransmission ratios on UE level and cannot differentiate between QoS flows using RLC AM or UM. Adding the HARQ or ARQ retransmission ratios to the maximum uplink RLC delay values skews the delay observation.
- When using the Uplink Configured Grant for URLLC Public RAN feature, the uplink delay for grants that empty the UE buffer is determined by the configured CG periodicity. In this case the CG periodicity represents the inter-scheduling delay of the uplink grants. For uplink grants that do not empty the UE buffer, the RLC delay counters measure the inter-scheduling delay between BSR requested grants like for dynamic scheduling.

The figure below shows the main delay factors for initial packet transmissions on a TDD carrier. The figure differentiates between Dynamic Grant (DG) scheduling at the top and Configured Grant (CG) scheduling at the bottom of the figure.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 49   Initial Packet Transmission Delay with Dynamic Grant and Configured Grant Scheduling

With dynamic grant scheduling, the uplink delay for the initial packet transmission of new data arriving in an empty UE buffer is determined by following components:

- SR Periodicity The configured interval when UE is allowed to send SRs. Depending on the TDD pattern and SR periodicity pool setup, the delay can range between 5 - 80 ms. For details, see Dynamic Grant Scheduling
- SR to Transmit The time to provide the UE with an initial grant for uplink transmissions after receiving the SR. This is mainly determined by the SR-2-Grant delay, which means the time the gNodeB needs to process the SR, encode the grant, and send it to the UE on PDCCH. The downlink slot including the grant on PDCCH must be separated from the uplink slot including the data on PUSCH according to configuration of the K2 constant. Note: The SR-2-Grant delay is not impacted when using connected mode DRX. UEs can send SRs irrespective of the DRX state they are in. The DRX inactivity timer (drx-InactivityTimer) starts after the UE has decoded PDCCH grant for the SR.

The contribution of the SR periodicity and SR-2-Grant delay to the e2e packet delay of services depends on the number of SRs which are transmitted during the lifetime of the service. The delay contribution is more significant for low data rate services where the UE buffer is emptied with one or a few uplink transmissions. It is less significant for high data rate services or if the air interface is the bottleneck and many BSR grants must be scheduled to empty the UE buffer. The number of samples collected by the PM and EBS-N counters: pmMacPucchSrPuschGrantLatDistr and pmEbsnMacPucchSrPuschGrantLatMaxDistr might provide some high-level indication about the SR ratio, although the SRs cannot be associated with a specific service flow.

With configured grant scheduling, the uplink delay for initial packet transmissions is determined by the:

- CG Periodicity The configured interval when the UE is allowed to send data in uplink slots. Depending on the TDD pattern and CG profile setup, the delay can range between 2.5 - 40 ms. Longer periodicities are supported with a single CG profile which configures the periodicity uniformly across all UEs in the cell based on the NRCellDU.configuredGrantPeriodicity attribute value. The choice between single and multiple CG profiles is controlled through the GNBDUFunction.configuredGrantConfMode attribute.

In summary:

- The PDCP Queueing Delay (D1) is the main contributor to uplink delay. The EBS-N counters: pmEbsnRlcDelayTimeUlMax and mEbsnRlcDelayTimeUlMaxDistr can be used to measure the D1-C sub-component and parts of the D2.1 component. D2.1 is only partially covered since the counters do not directly consider any retransmission overhead. The counters mirror the E2e uplink delay trend if the RAN is the bottleneck and can be used with dynamic scheduling and configured grant scheduling. The counters must be filtered for the network slice hosting the TCC services. If needed, they can additionally be filtered across QoS flows in the network slice.
- The gNodeB estimates the UE queueing delay based on information provided by the UE. It has no direct insight into the behavior of applications on the UE or how the UE schedules different QoS flows running in parallel. Therefore, the measurements provided by the pmEbsnRlcDelayTimeUlMax and mEbsnRlcDelayTimeUlMaxDistr counters might not always be closely aligned with the observed E2e delay performance of the service. TCC service flows should be separated from other service flows running on the same UE by placing them into an own LCG to mitigate the influence from non-TCC flows on the delay estimate.
- With dynamic scheduling, the SR periodicity and SR-2-Grant delay contributes to the overall uplink delay in the RAN. The contribution might be neglected for uplink heavy traffic or when the air interface is the bottleneck, and the delay is mainly determined by the PDCP queueing and inter-scheduling delay. The delay introduced by the SR periodicity and SR-2-Grant delay only relates to initial packet transmissions when new data arrives in an empty buffer. The SR ratio during a measurement ROP might be inferred from the number of samples recorded by the pmMacPucchSrPuschGrantLatDistr or pmEbsnMacPucchSrPuschGrantLatMaxDistr counters. Since SRs cannot be associated with specific QoS flows, the delay contribution of the initial packet transmissions cannot be determined for TCC services running together with other services on the same UE.

Aggregation of Delay Metrics

The uplink and downlink delay observed with PM or EBS-N counters can be aggregated over:

- Time To get the cell-specific, time aggregated Performance Indicator (PI), corresponding counters can be aggregated over certain periods of time. For example, the RLC delay related PI reported by the pmEbsnRlcDelayTimeUlMaxDistr and pmEbsnRlcDelayTimeDlMaxDistr values can be collected over several hours, days or weeks to calculate a weighted average maximum delay for all distribution bin samples during this period.
- Geographical Area PI related counters can be aggregated over a cluster of NR cells in a geographical area, consisting of one or several Tracking Areas (TAs).

However, aggregation might mask individual occurrences of unusual results in a specific time or location or might be subject to bias. For example, calculating the average delay of a NR cell across busy hour and non-busy hour times might hide delay violations during individual periods within busy hours. Moreover, aggregating the delay performance across an area using different cell types (FDD and TDD) creates a bias depending on the number of samples collected for each. To mitigate this, the aggregation can be done separately for busy and non-busy hours or frequency layers.

The figure below shows a simple aggregation approach, where the uplink and downlink EBS-N counters for the RLC delay distribution are aggregated per cell during busy hours.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 50   Aggregation of RLC Delay over Time

For the CIoT slice of each cell, the weighted average maximum RLC delay in milliseconds is calculated from the distribution counters by summing up the individual products of the bin center values and sample numbers and dividing the result by the total number of samples across all bins. Additionally, the overall maximum RLC delay value for the CIoT slice is collected for each cell. Both values, the weighted average maximum and the maximum RLC delay are then plotted for each cell in the geographical area as shown in the bottom right of the figure. No aggregation is done across the area to avoid the delay performance being biased by the cell type (FDD or TDD) or the distribution of UEs across frequency layers.

The graph on the bottom right shows the weighted average maximum and maximum delay for each cell during busy hours. The maximum delay value indicates whether the delay for the service flow is below or above the required RAN PDB, but not how often this maximum delay value is reached. The latter is indicated by the delta between the maximum and weighted average maximum values. The smaller the delta between both, the more samples are close to the maximum delay which indicates which cells might have performance issues during busy hour times.

For additional information about the aggregation of KPIs, see Key Performance Indicators.

#### 2.4.6.2 NR RAN Packet Loss

Packet loss is an important metric related to service integrity. The higher the packet loss, the higher the impact on the service performance related to throughput and latency. In NR RAN, packet loss might be due to signal interference, poor signal quality or congestion due to high load. If packet loss cannot be avoided, for example by means of link adaptation adjusting the coding rate based on the reported channel conditions and performing HARQ or ARQ retransmissions, loss-aware applications might retransmit packets and throttle the data rate. Congestion due to high load might trigger RAN traffic management functions, like Active Queue Management (AQM), which drop packets to avoid long standing queues. The L4S features in NR RAN help to control the queue length and reduce AQM packet drops for rate-adaptive services supporting it. For details about L4S, see  Flow and Congestion Control. For TCC services, it is recommended to observe the packet loss and drop rates in NR RAN together with the delay performance.

Note: The packet loss rate reflects the share of packets that are not received by the target while the packet drop rate reflects the share of packets that are not sent to the target due to traffic management during high load. The packet drop rate can be seen as a part of the packet loss rate.

The figure below shows the available EBS-N counters measuring the uplink and downlink packet loss on different gNodeB interfaces per NR Cell including F1-U and Xn-U. The downlink packet drops due to AQM traffic management are measured on NG-U interface level instead of NR cell level. For completeness, the counters are shown for a NR-NR dual connectivity scenario.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 51   Uplink and Downlink Packet Loss Components in NR RAN

The downlink packet drops due to AQM can be monitored with following EBS-N counter for each QoS flow on the NG-U interface:

- pmEbsnPdcpPktRecDlDiscAqm5qi, pmEbsnPdcpPktRecDl5qi The pmEbsnPdcpPktRecDlDiscAqm5qi counter is stepped when downlink PDCP SDUs are discarded by AQM. When using downlink L4S for rate-adaptive services, this counter should ideally report zero packet loss for the corresponding QoS flow. If non-zero values are reported, the downlink L4S and AQM thresholds might require a better alignment. The drop rate can be determined by dividing the counter value by the pmEbsnPdcpPktRecDl5qi counter value. Since the counters report discarded and received packets for MO instances representing NG-U interfaces, the NG-U drop rate must be correlated with NR Cell related performance measurements. For example, if a gNodeB connects to a single UPF serving all UEs in the CIoT network slice, there is a 1:1 relation between the NG-U and NR cell measurements if the EBS-N counters are filtered by the Snssai and the NR cell measurements are aggregated across all cells supporting the CIoT network slice on the gNodeB. If multiple UPFs are used, the NG-U measurements must also be aggregated.

The uplink and downlink packet loss over the F1-U interface is measured with following EBS-N counters:

- pmEbsnPktLoss{Ul,Dl}F1UDistr Most interesting is the counter for uplink which is stepped for packets that are lost or received out of order and discarded when the CUCP5qi.tReorderingUl and AqmCfg.tOooUlDelivery timers expire. For downlink, the counter reflects packet loss due to high load when buffers start to overflow and traffic management functions like AQM are either disabled or can no longer control the queue length. Note: The corresponding PM counter pmPdcpPktLossUlF1UQos measures the total uplink packet loss per DRB on CUUP function level and cannot be filtered per network slice.

The uplink and downlink packet loss over the Xn-U interface is measured like for the F1-U interface with following EBS-N counters:

- pmEbsnPktLoss{Ul,Dl}XnUDistr These counters can be ignored if NR-NR dual connectivity is not used for TCC services. They also do not measure the packet loss during Xn handover.

The packet loss over the NR-Uu interface cannot directly be measured. Nevertheless, occurrences of packet loss might be indicated by following EBS-N counters measuring the HARQ failure rate:

- pmEbsnMacUeHarq{Ul,Dl}FailRateDistr These counters are stepped if all HARQ retransmissions fail, for example due to poor signal quality. The counters report the loss on UE level and might be used to the estimate packet loss for TCC services by filtering them by the Snssai or UE group. Note: If the loss is due to poor signal quality, RLC retransmissions do not help mitigating the packet loss. In this case, the loss is applicable to QoS flows regardless of using RLC UM or AM.

Packet loss on the UE side cannot be measured unless the UE is capable to report M7 measurements to the gNodeB according to the MDT procedures in 3GPP TS 37.320.

# 3 RAN Feature List

This section lists some of the key NR features which are relevant for the Critical IoT in public RAN solution deployment.

| NR Feature                                                                        | Feature Type   | Package                                                    |
|-----------------------------------------------------------------------------------|----------------|------------------------------------------------------------|
| UE Grouping Framework (FAJ 121 5314)                                              | Basic          | NR Base Package (FAJ 801 4002)                             |
| QoS Framework (FAJ 121 5157)                                                      | Basic          | NR Base Package (FAJ 801 4002)                             |
| RAN Slicing Framework (FAJ 121 5095)                                              | Licensed       | NR Base Package (FAJ 801 4002)                             |
| Priority-Controlled Scheduling (FAJ 121 5192)                                     | Licensed       | NR Base Package (FAJ 801 4002)                             |
| NR Mobility (FAJ 121 5041)                                                        | Basic          | NR Base Package (FAJ 801 4002)                             |
| NR Traffic Steering (FAJ 121 5458)                                                | Basic          | NR Base Package (FAJ 801 4002)                             |
| User- and Service-Specific Mobility (FAJ 121 5399)                                | Licensed       | Intelligent Traffic Management and Mobility (FAJ 801 4022) |
| RRC Inactive State (FAJ 121 5328)                                                 | Licensed       | Intelligent Traffic Management and Mobility (FAJ 801 4022) |
| NR DL Carrier Aggregation (FAJ 121 5201)                                          | Licensed       | Peak Rate Evolution (FAJ 801 4005)                         |
| NR-NR Dual Connectivity (FAJ 121 5380)                                            | Licensed       | Peak Rate Evolution (FAJ 801 4005)                         |
| NR Carrier Aggregation Scheduling Optimization for External SCells (FAJ 121 5722) | Licensed       | Peak Rate Evolution (FAJ 801 4005)                         |
| NR Relative Priority Scheduling (FAJ 121 5293)                                    | Licensed       | Advanced RAN Slicing (FAJ 801 4019)                        |
| NR Rate-Controlled Scheduling (FAJ 121 5481)                                      | Licensed       | Advanced RAN Slicing (FAJ 801 4019)                        |
| NR Radio Resource Partitioning (FAJ 121 5337)                                     | Licensed       | Advanced RAN Slicing (FAJ 801 4019)                        |
| NR QoS-Aware Downlink Carrier Aggregation (FAJ 121 5571)                          | Licensed       | Advanced RAN Slicing (FAJ 801 4019)                        |
| Uplink Configured Grant for URLLC Public RAN (FAJ 121 5407)                       | Licensed       | Critical IoT for Public RAN Base Package (FAJ 801 1700)    |
| Downlink RAN-Assisted Rate Adaptation with L4S for Public RAN (FAJ 121 5664)      | Licensed       | Critical IoT for Public RAN Base Package (FAJ 801 1700)    |
| Uplink RAN-Assisted Rate Adaptation with L4S for Public RAN (FAJ 121 5750)        | Licensed       | Critical IoT for Public RAN Base Package (FAJ 801 1700)    |
| NR Service-Adaptive Link Adaptation (FAJ 121 5513)                                | Licensed       | Performance Boost (FAJ 801 4021)                           |
| NR Service-Adaptive DRX (FAJ 121 5325)                                            | Licensed       | Performance Boost (FAJ 801 4021)                           |
| NR Service-Adaptive User Plane Profile (FAJ 121 5640)                             | Licensed       | Performance Boost (FAJ 801 4021)                           |
| NR Service-Adaptive SR Periodicity (FAJ 121 5595)                                 | Licensed       | Performance Boost (FAJ 801 4021)                           |
| NR Adaptive Uplink Grant Size (FAJ 121 5729)                                      | Licensed       | Performance Boost (FAJ 801 4021)                           |

# 4 Example Network Designs

This section shows two Critical IoT deployment scenarios to describe the potential application of the TCC Toolbox features and concepts described in  Principles and Guidelines.

Both scenarios require the availability of URSP functionality in the UE and the 5GC Policy Control Function (PCF) to steer the service traffic flows into the network slice related PDU sessions. If URSP functionality is not available, UE local configuration might be used as alternative. Two example TCC services are considered:

- Cloud Gaming All service traffic types are associated with the default QoS flow using 5QI 7 in the cloud gaming related PDU session. If the service supports L4S, a dedicated QoS flow is set up to separate L4S from non-L4S traffic. For cloud gaming, L4S support in RAN is mainly enabled in downlink direction.
- Cloud AR All service traffic types are associated with the default QoS flow using 5QI 80 in the cloud AR related PDU session. The latency requirements are more stringent than for the cloud gaming service assuming that some multimedia processing is done in the edge cloud connected to the mobile network. If the service supports L4S, a dedicated QoS flow is set up to separate L4S from non-L4S traffic. For cloud AR, L4S support in RAN is enabled in uplink and downlink directions.

Frequency bands in FR1 providing sufficient capacity are used for the deployment which might be combined with NR carrier aggregation.

Following TCC toolbox features are prerequisites for both scenarios:

- QoS Framework
- UE Grouping Framework
- RAN Slicing Framework

For a more comprehensive feature information see  RAN Feature List.

## 4.1 Scenario 1: Dedicated Network Slice for Critical IoT

The figure below shows the end-to-end deployment for a scenario where TCC services are provided through a dedicated CIoT network slice in the public RAN.

The identifier for the CIoT network slice is a non-standard S-NSSAI and composed of the HDLLC slice type (SST 6) with an arbitrary SD value, for example 1. The identifier for the eMBB network slice is a standard S-NSSAI composed of the eMBB slice type (SST 1) without a SD value.

The table in the figure outlines an example setup for QoS, scheduling and radio resource partitioning. The UEs have subscriptions for both network slices and all services are accessible through the same device.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 52   Deployment Scenario 1: Dedicated Network Slice for Critical IoT

The central cornerstones related to the QoS, scheduling and radio resource partitioning setup include the following:

- The QoS flows for Internet-based data traffic (5QI 9) and IMS-based voice traffic (5QI 1 and 5Qi 5) in the eMBB network slice are scheduled with the Priority-Controlled Scheduling feature. Absolute priorities, that means Priority Domain values, are used for differentiation.
- The QoS flows for the cloud gaming (5QI 7) and cloud AR (5QI 80) services in the CIoT network slice are scheduled with the NR Rate-Controlled Scheduling feature and use the same absolute priority. Different relative priorities are configured to provide more scheduling opportunities for the cloud AR service. The QoS flow for the cloud gaming service is configured with a prioritized rate in downlink direction to elevate the scheduling priority to PD 46 if the configured rate is not achieved. The QoS flow for the cloud AR service is configured with a prioritized rate in uplink and downlink directions.
- If the cloud gaming and cloud AR services support L4S, dedicated QoS flows with operator defined 5QI values a and b are created for the L4S traffic. L4S support in RAN is only enabled for the dedicated QoS flows but not for the default QoS flows 5QI 7 and 5QI 80. Otherwise, the same RAN configuration is applied to the default and dedicated QoS flows in the PDU session of the respective service.
- The NR Radio Resource Partitioning feature is used to protect the 5QI 9 service by configuring a minimum radio resource share in uplink and downlink direction. If the partition is overutilized the 5QI 9 QoS flow is treated according to the default NR RRP scheduling priority DEMOTED\_NORMAL. Thus, it competes with the rate-controlled TCC QoS flows for the available radio resources in the same Priority Domain if the latter reach their prioritized data rates. If the 5QI 9 service flow should instead be treated with a lower priority during partition overutilization, the ResourceAllocationPolicy.demotedRrpSchedulingPriority attribute can alternatively be set to DEMOTED\_LOW.
- The VoNR QoS flows 5QI 1 and 5QI 5 are configured as prioritized traffic within NR Radio Resource Partitioning and exempted from being partitioned. Both VoNR QoS flows have the highest priority among all QoS flows and get prioritized access to radio resources during contention. They are always treated with the ELEVATED NR RRP scheduling priority.
- In the 5GC network dedicated UPFs are deployed for each network slice. The UPF for the TCC services is located in an edge cloud close to the RAN to minimize latency. Different Data Network Names (DNNs) are configured on the UPFs to provide a dedicated PDU session for each of the services. If the Data Networks connecting the different services of a network slice are accessible through different locations, different UPFs can be used for each Data Network in the Edge 5GC and Central 5GC. For example, there could be dedicated UPFs connecting the Low Latency and Very Low Latency Data Networks and dedicated UPFs connecting the Internet and IMS Data Networks.

Note: On the gNodeB, the network slices and QoS flows can be mapped to different Virtual Private Networks (VPNs) and Differentiated Service Code Points (DSCP) to facilitate the service differentiation over the transport domain.

The potential use of TCC toolbox features in NR RAN is summarized in the table below.

For the service differentiated feature configuration in NR RAN, four UE Groups are defined:

- Base UE Group Feature configuration for QoS flows not matching any of the other UE groups below. It is mainly applicable to the 5QI 9 QoS flow in the eMBB network slice.
- VoNR UE Group Feature configuration for the voice 5QI 1 QoS flow in the eMBB network slice.
- CIoT UE Group 1 Feature configuration for the cloud gaming 5QI 7 QoS flow. If dedicated QoS flows are set up in addition to the default QoS flow, the 5QI value of the default QoS flow is sufficient for the selection of the UE group. Differentiation between default and dedicated QoS flows is realized by customizing the feature configuration based on the 5QI values.
- CIoT UE Group 2 Feature configuration for the cloud AR 5QI 80 QoS flow. The 5QI value of the default QoS flow is sufficient for the selection of the UE group in case multiple QoS flows are established for the AR service. Differentiation between default and dedicated QoS flows is realized by customizing the feature configuration based on the 5QI values.

| TCC Toolbox Feature (NR RAN)                                  | Base UE Group                                        | VoNR UE Group                                            | CIoT UE Group 1 (Cloud Gaming)                                                                                             | CIoT UE Group (Cloud AR)                                                                                                   |
|---------------------------------------------------------------|------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|
| UE Group selection criteria                                   | -                                                    | snssai==SST 1/No SD, 5qi==1, rrcEstResCause==moVoiceCall | snssai==SST 6/SD 1, 5qi==7                                                                                                 | snssai==SST 6/SD 1, 5qi==80                                                                                                |
| NR Radio Resource Partitioning                                | Partition mapping with minimum UL/DL resource shares | Prioritized/exempted from partition mapping              | No partition mapping                                                                                                       | No partition mapping                                                                                                       |
| NR DL Carrier Aggregation                                     | Enabled                                              | Disabled                                                 | Disabled or enabled with PCell scheduling to mitigate Link Adaptation issues                                               | Disabled or enabled with PCell scheduling to mitigate Link Adaptation issues                                               |
| NR-NR Dual Connectivity                                       | Enabled                                              | Enabled with MCG use                                     | Disabled or enabled with either MCG or SCG use to mitigate Link Adaptation and packet reordering issues                    | Disabled or enabled with either MCG or SCG use to mitigate Link Adaptation and packet reordering issues                    |
| NR Mobility                                                   | Enabled                                              | Enabled                                                  | Enabled                                                                                                                    | Enabled                                                                                                                    |
| NR Traffic Steering                                           | Enabled                                              | Disabled                                                 | Enabled with a customized MLC periodic interval to reduce interruptions caused by cell set changes or handovers.           | Enabled with a customized MLC periodic interval to reduce interruptions caused by cell set changes or handovers.           |
| RRC Inactive State                                            | Enabled                                              | Enabled                                                  | Enabled                                                                                                                    | Enabled                                                                                                                    |
| QoS-Controlled SR Scheduling                                  | SR_PRIO_5QI handling mode                            | SR_PRIO_5QI handling mode with 5QI 1 condition           | SR_PRIO_5QI handling mode                                                                                                  | SR_PRIO_5QI handling mode                                                                                                  |
| NR Service-Adaptive Link Adaptation                           | Enabled with default BLER target                     | Enabled with a service-specific BLER target              | Enabled with a service-specific BLER target                                                                                | Enabled with a service-specific BLER target                                                                                |
| Link Adaptation for URLLC in Public RAN                       | Disabled                                             | Disabled                                                 | Disabled                                                                                                                   | Disabled                                                                                                                   |
| Uplink Configured Grant for URLLC Public RAN                  | Disabled                                             | Disabled                                                 | Disabled or Enabled with CG size=0                                                                                         | Enabled with CG size=0                                                                                                     |
| NR Service-Adaptive SR Periodicity                            | Enabled                                              | Enabled                                                  | Enabled. If the Uplink Configured Grant feature is enabled, the SR periodicity must be greater than the CG periodicity     | Enabled with SR periodicity > CG periodicity                                                                               |
| NR Service-Adaptive DRX                                       | Enabled with default DRX settings                    | Enabled with service specific DRX settings               | Disabled                                                                                                                   | Disabled                                                                                                                   |
| Downlink RAN-Assisted Rate Adaptation with L4S for Public RAN | Disabled                                             | Disabled                                                 | Enabled, if supported by the service. L4S thresholds configured according to the required downlink RAN PDB of the service. | Enabled, if supported by the service. L4S thresholds configured according to the required downlink RAN PDB of the service. |
| Uplink RAN-Assisted Rate Adaptation with L4S for Public RAN   | Disabled                                             | Disabled                                                 | Disabled                                                                                                                   | Enabled, if supported by the service. L4S thresholds configured according to the required uplink RAN PDB of the service.   |

## 4.2 Scenario 2: Shared Network Slice for Critical IoT

The figure below shows the end-to-end deployment for a scenario where TCC services share the eMBB network slice with other services in the public RAN.

The main differences compared to the Scenario 1 are the following:

- A common S-NSSAI value is used as selection criteria for the different UE groups. The UEs are grouped based on the 5QI value of the service flows as selection criteria.
- The AR and gaming service flows are mapped to an own radio resource partition that is configured with a resource share.
- The URSP policies signaled to the UE steer the traffic into different PDU sessions based on the given DNN information.

Otherwise, the scenario 2 is operationally identical to scenario 1 and the same differentiated TCC toolbox configuration is applied with the UE groups.

![Image](../images/371_22112-IPM10141_100Uen.V/additional_3_CP.png)

Figure 53   Deployment Scenario 2: Shared Network Slice for Critical IoT

# 5 Solution Guideline Change History

This section summarizes the major changes in this document release.

| Release   | Main Changes                                                                                                                                    |
|-----------|-------------------------------------------------------------------------------------------------------------------------------------------------|
| 25.Q1     | Added uplink L4S support and delay performance observability in RAN.                                                                            |
| 24.Q4     | No Changes                                                                                                                                      |
| 24.Q3     | Added flow and congestion control information including downlink L4S support in RAN.                                                            |
| 24.Q2     | Document renamed, Service-Adaptive SR periodicity and NR Service-Adaptive User Plane Profile added.                                             |
| 23.Q3/4   | Document restructuring. Providing guidelines for network slice deployments and differentiated configuration of TCC Toolbox features in the RAN. |
| 23.Q2     | No Changes                                                                                                                                      |
| 23.Q1     | No Changes                                                                                                                                      |
| 22.Q4     | The section “Balanced TDD Pattern for High-Band” added                                                                                          |
| 22.Q3     | RAN Sharing with AR UC introduced                                                                                                               |
| 22.Q2     | Link Adaptation for URLLC, Uplink Configured Grant (CG) for URLLC Public RAN                                                                    |
| 22.Q1     | First release: Critical IoT Solution Overview, Use Cases with or without URSP and Slicing                                                       |