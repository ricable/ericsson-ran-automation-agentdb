#!/usr/bin/env node
/**
 * Test ONNX local inference with Phi-4 model
 */
import { ONNXLocalProvider } from './providers/onnx-local.js';
async function testONNXLocal() {
    console.log('üß™ Testing ONNX Local Inference (Phi-4 CPU)\n');
    try {
        const provider = new ONNXLocalProvider({
            modelPath: './models/phi-4/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx',
            executionProviders: ['cpu'],
            maxTokens: 50
        });
        console.log('Test: Simple Inference');
        console.log('======================');
        const response = await provider.chat({
            model: 'phi-4',
            messages: [
                { role: 'user', content: 'What is 2+2?' }
            ],
            maxTokens: 20
        });
        console.log('\nüì• Response:');
        console.log(`  Text: ${response.content[0].type === 'text' ? response.content[0].text : ''}`);
        console.log(`  Latency: ${response.metadata?.latency}ms`);
        console.log(`  Tokens: ${response.usage?.inputTokens} in / ${response.usage?.outputTokens} out`);
        console.log(`  Cost: $${response.metadata?.cost}`);
        console.log(`  Providers: ${response.metadata?.executionProviders?.join(', ')}`);
        console.log('\n‚úÖ Test passed!');
        await provider.dispose();
    }
    catch (error) {
        console.error('\n‚ùå Test failed:', error);
        process.exit(1);
    }
}
testONNXLocal();
